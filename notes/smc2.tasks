{"desc":"(2:00?) #today #gce\n\n- [x] increase gce nodes from 5 to 8\n- [ ] add new cassandra nodes:\n   - [x] dc5\n   - [x] dc5: (re-)start all hubs and haproxy in dc5:\n   \n            [a.restart('hub', host='smc%sdc5'%i, wait=False) for i in range(1,9)] \n            [a.restart('haproxy', host='smc%sdc5'%i, wait=False) for i in range(1,9)] \n   \n   - [x] dc6\n   - [x] dc6: (re-)start all hubs in dc6\n   \n        [a.restart('hub', host='smc%sdc6'%i, wait=False) for i in range(1,9)] \n        [a.restart('haproxy', host='smc%sdc6'%i, wait=False) for i in range(1,9)] \n        \n   - [x] dc7\n   - [x] dc7: (re-)start all hubs in dc7\n   \n        [a.restart('hub', host='smc%sdc7'%i, wait=False) for i in range(1,9)] \n        [a.restart('haproxy', host='smc%sdc7'%i, wait=False) for i in range(1,9)] \n   \n- [x] spin up nginx, haproxy, stunnel\n\n\n...\n\n- [ ] should do nodetool clean in each dc","position":-1,"last_edited":1424274866304,"task_id":"0932dd45-2c9a-4d41-aa84-e85725b9df8f","done":1424274865896}
{"desc":"(2:00?)  #gce\nswitch to gce load balancing","position":0,"last_edited":1425611822838,"task_id":"d63d12de-c90e-4c7c-a2cf-3c26870e1746","done":1425611822426}
{"desc":"#invalid #gce\nrewrite hub/bup_server, etc., to be able to use Google Data Store (and Google Cloud Store for blobs) as an alternative to Cassandra.\n","position":1,"last_edited":1425503202592,"task_id":"8847b690-dd6f-4a5c-936b-d0b2f9d04fc0","done":1425503202176}
{"desc":"#today\n","position":2,"last_edited":1424143447313,"task_id":"b89d32f7-754a-4de3-b743-9275f0b94dcb","deleted":true}
{"desc":"make compute vm's at UW use much more RAM and all cores.","position":-0.5,"last_edited":1424993183695,"task_id":"62f0c1ee-d4ef-4c85-9913-96b3e226c5f6","done":1424993183279}
{"desc":"#6\nalternative: instead of using nginx, use this: https://cloud.google.com/storage/docs/website-configuration\n\nhttps://cloud.google.com/storage/docs/website-configuration#tip-dynamic\n\nrejected -- this will lock us in but with no real win.","position":-0.75,"last_edited":1427287317984,"task_id":"ad05d59a-90ae-4984-af7f-1f36d1fd12c7","done":1427287317572}
{"desc":"#1 #bug #editor #sagews\ncorruption.\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14b9852c4c20ed4b\n\n    I can see that in the history around revision 294.  The list of changes is stored in the history file, which I've copied and will look at closely.\n\n    What web browser, operating system, and are there any javascript console errors?\n\n    https://cloud.sagemath.com/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/files/tmp/Balbusaur.sagews.sage-history\n\n\nSolution/idea:\n\n- replace sync by a *local* sync on the cell level / do same with syncdb, ipython, etc.  That would fix a ton of problems.  A sort of sudo op transform...?","position":-0.875,"last_edited":1424791516590,"task_id":"7bf29e20-ec21-430e-b6b4-a86e9fdb419d","done":1424791516187}
{"desc":"#1 #today\nincrease quota from 10MB to 32MB for public files.","position":-0.8125,"last_edited":1424274824854,"task_id":"97322cae-1191-45b6-8b2b-83673e26455a","due_date":1424234908506,"done":1424274824438}
{"desc":"#1 #today \ndeal with compute firewall issues once for all.","position":-0.78125,"last_edited":1424286780506,"task_id":"c14cf799-27fa-44fe-9888-1c2068e4c448","done":1424286780104}
{"desc":"(1:00?) #0 #today\nrewrite code to make it possible to properly rebuild that email_address index, then run it. \n\n\n","position":-2,"last_edited":1424285604487,"task_id":"df7d7a00-65a2-43a9-8ecf-8e58c8f9c20b","done":1424285604073}
{"desc":"(1:30) #0 #today\nlocal_hub call just hangs sometimes....\n\nI think I fixed this -- we shall see.\n\nNew data: this may be caused when a project **moves**.\n\n    2015-02-18T15:49:24.809Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:49:24.809Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:49:24.809Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:49:49.813Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:49:49.814Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:49:49.815Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:12.875Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"touch\",\"args\":[\".sag...\n    2015-02-18T15:50:12.876Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:12.876Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:14.813Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:50:14.814Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:14.814Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"\n    2015-02-18T15:50:39.821Z - debug: hub <-- client (client=QAJYGDE8p7f3UfAeAADa): {\"event\":\"project_exec\",\"project_id\":\"80496c4c-c705-46f5-b04f-34e7c031b21d\",\"path\":\"\",\"command\":\"git-ls\",\"args\":[\"--t...\n    2015-02-18T15:50:39.822Z - debug: project(80496c4c-c705-46f5-b04f-34e7c031b21d): call\n    2015-02-18T15:50:39.823Z - debug: local_hub(80496c4c-c705-46f5-b04f-34e7c031b21d): \"call\"","position":-3,"last_edited":1424886185364,"task_id":"86b4e172-da35-4741-b3ae-8e53e68c689c","done":1424886184955}
{"desc":"(0:30?) #gce #low-usage\nnodetool clean in each dc","position":-0.9375,"last_edited":1425611820100,"task_id":"35ede044-dc1d-479c-b8d8-1b0afcafe2e9","deleted":true}
{"desc":"(2:00?) #now #upgrade #today\nupgrade to sage-6.5 and sync out","position":-6.856689453125,"last_edited":1424753284301,"task_id":"7438c090-816a-490a-942e-ecd926e97350","done":1424753283887}
{"desc":"#today\nsetup new devel machines on sagemath, inc. infrastructure","position":-8,"last_edited":1424291393896,"task_id":"de7b6d2c-90b4-4bc2-a3aa-4613703d4259","done":1424291393489}
{"desc":"#today\nupdate my local virtualbox devel environment so I can code on the plane, etc. if I want.","position":-7,"last_edited":1424290639195,"task_id":"fbae0512-f889-4790-aa46-d5f582cf8cd0","done":1424290638777}
{"desc":"#now (1:00?) #vm #upgrade #install #today \n\n- [x] push it out\n- [x] as root in /root: `./update_salvus`\n- [x] `umask 022` then `sage -sh` then `pip install mrjob boto pattern seaborn`\n- [x] `apt-get update; apt-get dist-upgrade; apt-get autoremove`","position":-6.85675048828125,"last_edited":1424752389753,"task_id":"66ae9142-271b-4187-9c00-600ae964b28b","done":1424752389343}
{"desc":"(1:00?) #gce\ntest google datastore:\n\n- [ ] how the latency really is \n- [ ] compare with my cassandra latency\n- [ ] get a feel for using it.","position":-6,"last_edited":1425611717199,"task_id":"8cfcf70b-e99e-4fcc-8bee-4c088f5b9f37","done":1425611716784}
{"desc":"(1:00?) #gce\ntest google cloud storage for sharing files. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n - see https://cloud.google.com/storage/docs/website-configuration)","position":-6.75,"last_edited":1424993168197,"task_id":"15abe7db-fea5-4882-b336-4fe2febba40c","done":1424993167778}
{"desc":"(1:00?) #gce\ntest google load balancer...\n","position":-4.75,"last_edited":1425611719237,"task_id":"42816ea3-fc4a-41be-a0b1-b18457d68172","done":1425611718828}
{"desc":"(2:00?) #gce\nlearn/test switch to google DNS (instead of route 53)\n\nIt turns out that google dns is way behind route 53 in functionality, as keith said.\n\nSo instead I properly setup route 53, so now when a region fails another will automatically take over, etc.","position":-6.5,"last_edited":1425088297010,"task_id":"9a0c6905-655c-41fa-85cb-b76254bc55bb","done":1425088296598}
{"desc":"(4:00+) (1:30?) #check #0 #gce  #backup #today\n\nCome up with better approach that involves first rsyncing everything from the VM's and then (and only then) (possibly) running bup.  Right now bup uses too much RAM.\n\ncreate new automated back-up databases from cloud.\n\nPlan: \n\n0. [x] create zfs snapshot of filesystem across cluster\n\n   [x] delete all snapshots and backups directories:\n   \n     \t cd /mnt/cassandra/lib/data && time rm -rf */*/backups */*/snapshots\n      \n   [x] create new nodetool snapshot:\n   \n         nodetool snapshot\n\n1. [x] create backup entirely on a backup vm in same zone using cassandra snapshots/backups as usual\n    - [x] decide on a new PD size\n    - [x] create PD, format, mount\n    - [x] modify existing backup script\n\n2. [x] rsync the backup to bsd\n\n3. [x] rsync from bsd to other offline storage later...[ ] ","position":1.8125,"last_edited":1424459669181,"task_id":"34d3948a-9ee6-4a8b-97e1-a9be8b118ff1","done":1424459668769}
{"desc":"#gce\nbrand new gce machine template with sage-6.5 and minimal image size.","position":-2.25,"last_edited":1425611722555,"task_id":"681dafa5-b288-4d5a-9a96-ed10f6c59342","done":1425611722143}
{"desc":"(0:30) (0:45?) #now #today\nAdd another thing to home screen to add collaborators to encourage viral growth: \n\n\" Create or Import a File, Worksheet, Terminal or Directory...\"","position":1.75,"last_edited":1424395945486,"task_id":"5e71fc53-f186-4e07-b565-75555b9bcb2b","done":1424395945070}
{"desc":"(2:00?) #gce #cassandra #today\nreplace the cassandra node filesystems -- all of them -- with much smaller ext4 filesystems","position":1.625,"last_edited":1424390763273,"task_id":"fee727a9-9b11-425c-beea-51a274435641","deleted":true}
{"desc":"(1:00?) #security #6\ninvestigate this security scanner:\n\n   http://googleonlinesecurity.blogspot.com/2015/02/using-google-cloud-platform-for.html","position":1.875,"last_edited":1426254427633,"task_id":"1e15e912-1a6b-45ed-b1aa-baece5388c18"}
{"desc":"#gce\nseriously think through options for separating compute from storage\n\nIdeas:\n\n- store all snapshots of project in google cloud storage as a bup repo\n- when project isn't used for a while, delete it from compute vm or move to slower storage\n- when start project\n\n\n","position":-6.625,"last_edited":1425611714598,"task_id":"8730470e-01c1-4a9a-930a-9f88fd5fe451","done":1425611714188}
{"desc":"(1:00?) #upgrade\nupgrade to codemirror 5.0\n\nhttp://codemirror.net/codemirror-5.0.zip","position":-6.6875,"last_edited":1424468345411,"task_id":"8e8f3a41-75ec-4d02-91f4-8963afbc1055","done":1424468345411}
{"desc":"(1:03) #now (1:00?) #today\nget rid of password confirm and combine first/last name\nto reduce friction.  \npassword reset works fine.","position":-6.8125,"last_edited":1424472220316,"task_id":"a72958f2-e731-47e7-ae58-0571cbd8d331","done":1424472219915}
{"desc":"","position":-6.84375,"last_edited":1427813677083,"task_id":"f2e40a7f-9e59-427f-b65a-ff928ea113ff","deleted":true}
{"desc":"(1:00?) #today #now\ninvestigate longterm saving projects to google cloud storage via bup or https://github.com/moinakg/pcompress\n\n- Store bup repo of snapshots of project by simply updating, using rsync, and copying any files that might change.\n\n- Also use pcompress (?) to store complete copy of project as a single file, just in case (?).  Incremental based on timestamp (?).\n\n> HSY: except for the encryption (which is orthogonal anyways), this pcompress sounds similar to `lrzip`. It's an \"older\" standard linux utility and hence easily installed via apt-get install lrzip. `lrztar <directory>` and `lrzuntar` are all you need, see `man lrzip`. BTW: it compresses the sage binary tarball 7.6:1 in a few minutes (tried it for the last 6.5 release for ubuntu 14.4), which probably means it is mostly limited by I/O, not CPU.\n\n> William: I think bup is the best choice anyways.  I can't think of anything similarly efficient to solve this problem...\n\n- The current total space used by all bup repos is about 1.3TB, which would cost \\$34/month to store in Google cloud storage. \n\n- I think this isn't a viable approach since compute will often not be a GCE, but this must be at google... (?).\n\nOther ideas to test:\n\nIn each DC have a single global dedup'd compressed ZFS pool with rsync of all projects and also all bups.  It's fine if it is fairly slow. \n\n- Define the dream API I want, then list different ways of implementing it with different backends.\n- I can use that api to save/restore the bup repos to get fine-grained snapshots if I want.\n\nAPI: \n\n        storage save path     # efficiently saves changes to path since last save\n\n        storage restore path  # efficiently restores path from last save.\n\nSolutions:\n\n- rsync to/from a single big volume\n- rsync to/from distributed volumes with a cassandra index\n- rsync to/from gluster (or other cluster) filesystem\n- google cloud storage with incremental tarballs and repack every so often.\n- google cloud storage using bup repos and gsutil rsync\n- many individual persistent disks?!  (massively wasteful)\n- sparse image files stored in google cloud storage\n\nOR... don't change anything but just improve my current implementation to do solve additional problems:\n\n   - disk space running out: rebalance, increase volume size.\n   \nThe ultimate would be:\n\n   - a magic single multi-data-center global distributed filesystem that compute machines can mount, which is super fast.  Involves a local cache.\n   \nTest this:\n\n - glusterfs us in my sagemathcloud project on 3 nodes\n - another glusterfs in europe in my sagemathcloud project on 3 nodes\n - could then make the \"open project on compute\" process first check\n   for cached local files, and if not copy them from the glusterfs.\n   If so, just update them.  With state stored in cassandra (?) or filesystem.\n - then I could delete any projects from compute vm's without any loss in functionality (just speed), which since \n - could also move a project from any vm to another (even in same dc) without loss of files, just speed.\n - would just have to make the \"write to gluster\" operation write to all glusters via an rsync in parallel\n \n \nNone of the above solves any problems really.  Instead I should just improve the current system, which is pretty damn good -- just not finished.  I can decouple compute and storage enough this way. \n\n - add command to move all data for a project from one node to another in the same datacenter\n - make bup_repair properly get results in small batches rather than all at once.\n \n \nOr... the other option, which is:\n\n- In each DC have a single NFS server, and have all compute vm's mount from it over the LAN.\n- It will be a single zfs pool (so can snapshot) -- about 4TB for starters (could try to dedup compute data if possible... if that fails, rebuild without dedup.)\n\nNO.  That centralizes everything, which doesn't scale.  Even now it would barely work at UW.\n\n","position":-6.859375,"last_edited":1424661119535,"task_id":"e9b674f3-a438-473d-aa33-a90d5779005c","done":1424661119126}
{"desc":"(0:38) (0:10?) #today\nraise password reset email timeout to 1 hour.\n","position":-6.9296875,"last_edited":1424548524194,"task_id":"006a97da-3c28-433b-8ee8-ba5069a8a7cf","done":1424548523779}
{"desc":"(0:15?) #today\nchange collaboration link to go straight to \n","position":-6.84765625,"last_edited":1424460022382,"task_id":"3620595e-8d88-4446-9b65-09dcea44093e","deleted":true}
{"desc":"(3:00?) #terminal #feature #5\nterminal menu bar\n\nIdeas here: https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14b9843b3d96f348","position":-6.85546875,"last_edited":1427813652415,"task_id":"ddfadeda-e4bd-4ef1-81d4-abd473a739f4"}
{"desc":"#now (0:07) (0:30?) #upgrade #today\nupgrade to font-awesome 4.3.0","position":-6.85693359375,"last_edited":1424726001104,"task_id":"6ef80028-e10b-47b4-b5dc-07f887bf71e3","done":1424726000702}
{"desc":"#bug #ipython\ntoo many servers getting left around\n\nSee https://mail.google.com/mail/u/1/?pli=1#inbox\n","position":-6.89453125,"last_edited":1424726574297,"task_id":"e65ddfab-a356-4150-ada8-6e6a0d612797","done":1424726573888}
{"desc":"#now #today\n(1:30?) #gce\nsave money -- replace all cassandra disks by new 50GB ext4 format disks, and delete any log disks.\n\nThis must have taken 5 or 6 hours.  Very tedious!\n\n\n\n\t\n    sudo su\n    echo \"UUID=`ls -l /dev/disk/by-uuid/|grep -v sda|awk '{print $9}'|xargs` /mnt/cassandra   ext4 defaults 0 1 \" >>/etc/fstab\n    echo \"/home/salvus/salvus/salvus/scripts/restart_tincd; sysctl -w vm.max_map_count=131072\" > /etc/rc.local\n \tkillall java && sleep 10 && rsync -axvH /mnt/cassandra/ /mnt/x/ && sudo shutdown -h now\n \n on startup\n \n \tsudo chown salvus. -R /home/salvus/salvus/salvus/data/logs\n    sudo rmdir /mnt/x\n    \nFrom admin ipython:\n \n     h='smc5dc5'; [a.start(s, host=h, wait=False) for s in ['cassandra', 'stunnel', 'haproxy', 'hub', 'nginx']] ","position":-6.876953125,"last_edited":1424661104346,"task_id":"df18c746-be80-438e-9f3f-0bffaf943bc3","done":1424661080145}
{"desc":"#3\nconcerns -- what happens if copy files (using rsync) to a project (e.g. sending homework), but that project is not on?  \n\n- do files get properly saved ever?  \n- Will \"it\" just forgot and randomly loose files?\n\n> My memory is that this is already handled perfectly, but I want to double-check...","position":-6.857421875,"last_edited":1427813451370,"task_id":"0c2e72f9-18bd-46ac-8a60-506b838f45d7","done":1427813450967}
{"desc":"(1:15?) #0 #monitor #critical\nfix monitor script so it can run on GCE rather than UW.\n- [ ] modify conf/services script to get rid of all old nodes\n- [ ] (easy) write to database via hostname (not using tinc network)\n- [ ] (harder) send email (can't use gmail; use sendgrid but via python)","position":-0.7585754990577698,"last_edited":1430598504679,"task_id":"a196661c-4ded-4abe-baa9-ba293b2a6b88"}
{"desc":"","position":-6.85595703125,"last_edited":1424707490941,"task_id":"0c148baf-fc39-4a39-9491-da30065e7647","deleted":true}
{"desc":"(0:10?) #today\nexport a public version","position":-0.76953125,"last_edited":1424709582231,"task_id":"19468fef-d3ba-4e97-9c0b-630cefc824fe","done":1424709581826}
{"desc":"(0:05) (0:15?) #today #install #now\nsome pip install requests\n\nhttps://mail.google.com/mail/u/1/#inbox/14bb082788a79dfc","position":-6.8565673828125,"last_edited":1424726569197,"task_id":"5a819142-d614-4476-86b9-692b63c1aecf","done":1424726568791}
{"desc":"(2:00?) #feature #ui #5\nimprove the project log\n\nsome ideas  -- https://mail.google.com/mail/u/1/#inbox/14bad8af5d04134d\n\nshould we get rid of the files/projects/messages buttons in the log -- I don't think they are anything but confusing?","position":-6.85650634765625,"last_edited":1427813673987,"task_id":"38037606-4819-4a1e-8586-8fe998e3506b"}
{"desc":"(1:30?) #bug #install #4\nfix fenics install\nhttps://mail.google.com/mail/u/0/#inbox/14bb737b3d010595\n\nAnother vote for it here: https://mail.google.com/mail/u/1/#inbox/14c68181958a1e45","position":-6.8568115234375,"last_edited":1427813622226,"task_id":"7c42ca95-7450-492f-b2d0-7f45860ddcf8"}
{"desc":"(2:16) (0:40?) #0 #upgrade #today\nmajor new cassandra driver: https://mail.google.com/mail/u/0/#inbox/14bb78b75048c585\n\n- problem: weird stuff with uuid's/buffers and data types breaking the driver...","position":-6.85723876953125,"last_edited":1424843719149,"task_id":"a446fb16-b230-4734-8489-709186b5abe1","done":1424843718743}
{"desc":"(1:00?) #1 #today\nmake this url display terms of usage, read from a static file (?)\n\n  https://cloud.sagemath.com/articles/terms","position":-6.856903076171875,"last_edited":1427215960795,"task_id":"7a7906f6-d8c4-4408-a3cd-725acf85429c","deleted":true}
{"desc":"(1:00?) #admin\ntry to get google cloud monitoring/logging to work","position":1.9375,"last_edited":1425503209396,"task_id":"f77ced38-65c2-4f8b-81ac-4f89179b990d","done":1425503208977}
{"desc":"#today #now (0:05) (0:10?) update usage numbers","position":-0.7578125,"last_edited":1424726274687,"task_id":"8b3807a3-0691-4679-86fa-8f096ed43d57","done":1424726274271}
{"desc":"(1:30?) #vm #upgrade #install #1\n\n","position":-0.76171875,"last_edited":1427286422993,"task_id":"d236f82b-5f81-4495-8d37-ae5cd2a90258","deleted":true}
{"desc":"(3:30) (1:00?) #0 #today\nmake it so that changing the password invalidates all remember me cookies for a given user.\n\n- [x] (0:34) make a plan\n\nWe have a big (about 70K entries) table right now that has all the cookies along with the account_id as a value.\n\nIf we come up with a new approach that uses the same cookies, we can iterate through that huge table of all cookies and read it, then write to a new table.\n\nHow about this:\n\n- [x] (0:27) (0:20?) Whenever we *create* a remember_me cookie, we also write the key of that cookie to a set attached to the account.  `alter table accounts add remember_me          set<varchar>;`\n- [x] (0:23) (0:20?) Whenever we change the password, we delete all cookies with keys in that accounts table, thus invalidating them.\n- [x] (1:13) (0:20?) Every 5 minutes (?) the hub will check that the cookie used to login a connected user is still valid.  If it disappears from the table, the user is logged out.  This means implementing a message from hub to client that logs a user out.\n- [x] (0:58) (0:45?) #now Once above is working, we run through the entire 70K big table of existing cookies and write them to the accounts table, so that the above works.\n\n","position":-0.7587890625,"last_edited":1424915734457,"task_id":"0f753947-13da-46be-a63c-c491d3592941","done":1424915734051}
{"desc":"(0:55) #now (1:00?) #terminal #bug #today\noutput doubling race condition\n\nhttps://mail.google.com/mail/u/1/#inbox/14bbbc497857c33c\n\nDid some things -- not sure if this will fix it or not.  If I ever hit it again, i need to know if it is entirely client-side by testing another console connected, looking at server logs, etc...","position":-6.8572540283203125,"last_edited":1424823566664,"task_id":"4ea24d63-885b-4e55-9d31-c7f9331ebac2","done":1424823566257}
{"desc":"(2:22) #now (0:30?) #database #admin #2 #today\nrewrite bup_repair to stream output rather than do one big query\n\nThis took much longer than expected since I did it *right* with integrated streaming support, which is used also for the user search functionality (which was about to die). ","position":-6.8572998046875,"last_edited":1424900938476,"task_id":"49b0ce1a-bd86-4541-8060-7a8a229c8515","done":1424900938074}
{"desc":"(2:00?) #com #1\nmake it so users can move their project to an available location of their choice.\n\n- [ ] first between UW data centers\n- [ ] then to nonfree data centers","position":-0.75860595703125,"last_edited":1429157083527,"task_id":"52fe0cf8-0a34-4c63-9141-9ec83cb54fe3"}
{"desc":"(0:28) (1:00?) #now #today #bug #0\nfix bug in load of remote files\n\nreport: https://mail.google.com/mail/u/0/#inbox/14bbd4b67dc9e275\n\nto sage-devel: https://mail.google.com/mail/u/0/#sent/14bbd62bdc92cec1?compose=new","position":-6.857269287109375,"last_edited":1424813067674,"task_id":"8dfedcd1-a7b6-4345-b79c-1564fb43a839","done":1424813067271}
{"desc":"(0:15?) #monitor #4\nmake check_hub_* also check for \n\n\tgrep \" Uncaught exception:\" hub*.log\n","position":-7.444186329579679,"last_edited":1428537028038,"task_id":"9ffbf978-c690-4601-a00d-51720d3492d1"}
{"desc":"(2:30) (3:00?) #today #bug try/test -- rewrite how #ipython sync works to use the syncdb infrastructure (which itself could get improved later).  \n\nMotivation: https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14bc33cc2777f52e\n\nCurrently my \"sync friendly\" version of the ipynb file (the syncdb file) is basically NOT sync friendly, especially as the ipython notebook cell format has got a lot more complicated.  Ideas?\n\nRight now it looks like sync's that result in cell contents that are invalid json just result in an error.  But the client that caused that error doesn't know to retry. Think this through.  Possibly consider using a json patching to upstream... then use usual mechanism, i.e., some notion of purely local operational transform, repeated until it succeeds.  Could also apply to syncdb.","position":-0.75830078125,"last_edited":1425920139303,"task_id":"bc41b5a4-3208-4a35-86a2-97047e0a3d44","done":1425920138896}
{"desc":"(2:00?) #2 #database #backup\nplain text backups using database streaming.\n\nuse the streaming functionality of cassandra to make periodic \"proper\" CSV/text backups of the non-binary content \nof the database; can be nice to query from grep, etc.","position":-0.758544921875,"last_edited":1428069825899,"task_id":"6510c04c-bc2d-4797-a069-3135587d7828"}
{"desc":"(0:15?) #bug #5\nwhy does this project have 5 locations?\n\n      { project_id: '9a1eb6c1-9b97-48d8-aa42-306cb460439c',\n        bup_location: '3056288c-a78d-4f64-af21-633214e845ad',\n        bup_last_save:\n         { '3056288c-a78d-4f64-af21-633214e845ad': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           '630910c8-d0ef-421f-894e-6f58a954f215': Wed Feb 25 2015 21:05:14 GMT+0000 (UTC),\n           '94d4ebc1-d5fc-4790-affe-ab4738ca0384': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           'a7cc2a28-5e70-44d9-bbc7-1c5afea1fc9e': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n           'eec826ad-f395-4a1d-bfb1-20f5a19d4bb0': Wed Feb 25 2015 22:49:48 GMT+0000 (UTC) },\n        source_id: '3056288c-a78d-4f64-af21-633214e845ad',\n        timestamp: Wed Feb 25 2015 22:49:48 GMT+0000 (UTC),\n        targets:\n         [ '3056288c-a78d-4f64-af21-633214e845ad',\n           '630910c8-d0ef-421f-894e-6f58a954f215',\n           '94d4ebc1-d5fc-4790-affe-ab4738ca0384',\n           'a7cc2a28-5e70-44d9-bbc7-1c5afea1fc9e',\n           'eec826ad-f395-4a1d-bfb1-20f5a19d4bb0' ] } ]","position":-0.7586669921875,"last_edited":1427838576044,"task_id":"a2f80f7d-1969-4229-987d-9d1a96d428db"}
{"desc":"(1:00) #latex #bug #today\npages get randomly re-ordered, etc. when editing latex documents.  Instead, clean this up:\n\n- create one preview html div on load for each document page, as soon as we know number of pages\n- when changing state of a page (loading new version), show a spinner or something\n- always get the actual pages right","position":-0.758056640625,"last_edited":1424932758845,"task_id":"8e1ba602-a6c4-488b-827f-355f4aecb275","done":1424932758428}
{"desc":"#ipython #bug\n","position":-0.7579345703125,"last_edited":1424929208276,"task_id":"b59b023d-9eee-41d1-bf43-c199bc870f9b","deleted":true}
{"desc":"#bug #2 \ncode folding in latex documents now broken in new codemirror.\n\ne.g., in this file: https://cloud.sagemath.com/projects/50c2cd00-d0b7-4b9d-858a-657b587c0767/files/revision-critical-paper/chen.tex","position":-7.5,"last_edited":1425611450995,"task_id":"d6a1d123-60d0-45c0-b9cd-55edbd17bf69","done":1425611450587}
{"desc":"(0:15?) #com\nmachine type in database\n\n- add new database column to storage_servers indicating whether the machine is free or premium (or dedicated... needs to be flexible)","position":-6.856874465942383,"last_edited":1426253663552,"task_id":"065db1d1-a137-4c1d-864b-9d1803311369","deleted":true}
{"desc":"(1:00?) #com #2 #unclear\n- [ ] start one testing premium machine running at us-central1f on academic GCE project.\n- [ ] add to storage_servers table.","position":-7.444186328793876,"last_edited":1427286526207,"task_id":"be9e7714-14c7-40af-8f38-8a05ae3209d6"}
{"desc":"#0 #com #unclear\nadd function to bup_server to make a project premium, e.g., `premium_replication` which can either enable or disable replication of the project to premium machines in dc's. \n\nWill need something in database that indicates *what* is happening to the project, so multiple hubs don't compete to break it in half and make it vanish from existence.\n\nenable:\n\n- [ ] ensures it is replicated to one premium machine in each dc, if there are any\n- [ ] removes from all non-premium machines\n\ndisable:\n\n- [ ] make replicated to choice of non-premium machine in each dc that has one\n- [ ] removes from premium machines\n\nWhen user tries to do anything with a project that is getting upgraded/downgraded, show them an appropriate status message.","position":-6.856822967529297,"last_edited":1427737605314,"task_id":"16faee40-0392-4396-bab5-12d83074095f","done":1427737604907}
{"desc":"#com #1 #unclear\nbup_storage.py defaults\n\n(temporarily at very beginning, could just hard code something.)\n\n- [ ] add different default parameters to bup_storage.py for when a project starts on a premium machine versus a free machine.  This could just be a command line option that is passed to bup_storage.py from bup_server.  And it could be an option shen starting bup_server (?).  Or it could an option that is passed to bup_server with the start message.\n\n- [ ] alternatively, completely remove defaults from bup_storage.py.  They should instead be passed as options (from hub, got from database). ","position":-7.444186329317745,"last_edited":1427286350319,"task_id":"2f0680db-f7d2-429e-8279-c6bcad3d574e"}
{"desc":"#com #database\n\n- [ ] add column in database to projects that is called 'plan'.  this has\n    - account_id of the user that is paying for this plan\n    - the current plan name itself, e.g., 'premium'\n    - log: append-only list of objects '{timestamp:..., action:}' or maybe just a map from timestamps to actions, where the actions are 'start', 'stop'.\n    - include plan name in project information, so client can display it somehow, at top in project listings with better icon, etc.\n\n- [ ] add a column in database to accounts that is called pay_projects.  This is a set of project_id's of projects that the user is paying for.\n\n- [ ] add a column in database to accounts that has payment information.  This would be a token that identifiers the user's account at STRIPE.\n\n- [ ] new table in database called \"customer_activity\" that has columns:\n\n\t- timestamp_uuid as primary key, so can easily query by date range (is this even possible -- if not use a uuid and a date string??)\n    - account_id\n    - some sort of description of activity\n\n- [ ] Add functions to cassandra:\n\t- set_plan(plan, account_id)   where plan is one of 'free', 'premium'.\n      which will put something in all of the relevant tables above.\n    - is_paying_customer(account_id)  -- whether or not user is a paying customer (so cc number is known)","position":-6.856880187988281,"last_edited":1426253608531,"task_id":"c879fae5-e210-4f07-8968-d5eb2230de83","done":1426253608126}
{"desc":"#com #4\nif a user is paying for the project, don't allow them to be deleted from the project.","position":-6.856812953948975,"last_edited":1426253027324,"task_id":"bcec95e7-61a4-49fc-a058-bfa3c79bba56","deleted":true}
{"desc":"(2:30?) #0 #today\nsingle sign on/oauth2, etc.\n","position":-6.856813669204712,"last_edited":1427287465388,"task_id":"ec51f454-c972-439c-ad71-2a5288d1396c","deleted":true}
{"desc":"(2:00?) #com #0\n\n- [ ] update base template vm for machines\n- [ ] add one high-mem premium machine to each of dc5 and dc6 with 50GB SSD zpool for /projects, and slower zpool for bup repos.","position":-6.856845855712891,"last_edited":1426736873819,"task_id":"c6bac0c9-64d7-41fa-a179-fbcbd7f5feee","deleted":true}
{"desc":"#billing #stripe #today\n","position":-6.856900215148926,"last_edited":1425434827912,"task_id":"36a56a48-630d-4acc-9244-67d809dee8d4","deleted":true}
{"desc":"#gce\nprobably just disable the asia dc -- there is just so little usage. Not sure.","position":-6.85688591003418,"last_edited":1425611701537,"task_id":"f80a96f8-4252-448e-adb3-c1920fcf2fb3","done":1425611701121}
{"desc":"#billing #stripe #1\nmeta task\n\n- [ ] (0:20?) make https://william.wstein.org/#billing work\n\n- [ ] (1:30?) Also make it so that right when creating a project you get an easy option to create it as nonfree and even enter your credit card there if not done yet.  (So at least three chances/places to upgrade all integrated into the UI....)\n\n- [ ] (1:30?) get backend part of project move to work\n\n- [ ] (1:30?) get backend part of project move to respect subscription settings.\n\n\nthoughts: actually make subscription useful (e.g., move project, etc.):\n   - make we could add a notion of \"resources\" to a project, namely server(s) on which project can be run, which have certain quotas.\n   - for-pay projects will get *option* of running on different resources, which will change their quotas\n   - resource display will show current sync state of everything, load, etc.\n\nOptions:\n\n- Right when you open a project, instead of seeing Files..., you see a homepage for the project, with status information about it.  This could be a re-organized settings page with a pane about the project's location/state.\n\n- Just add a pane to project settings about project location.  Move it later.\n\nhttp://silviomoreto.github.io/bootstrap-select/\n\n   \n   \n\n- [ ] (1:00?)  function in hub to determine what subscriptions definitely really apply to a given project:\n - consult what's in database\n - then query stripe for confirmation (?)\n     \n\n## version 1 (usable by public to do something?)\n\n- [ ] (1:00?) for **now** only allow the owner to pay for a project or move it (change later)\n- [ ] (0:30?) when making card, put in default name based on their name, etc.\n- [ ] (1:00?) require password for some billing-related api operations.\n- [ ] (1:00?) admin setting so admins can change the stripe api keys\n- [ ] (1:00?) display subscriptions -- button to show more\n- [ ] (1:00?) display charges\n   - more details of each\n   - button to show more\n- [ ] (2:00?) non-US currency\n- [ ] (1:00?) define our first subscription.\n- [ ] (1:00?) replace ugly bootbox dialogs with nice modals in account billing page.\n\n## version 2 (polish)\n\n- [ ] a very simple summary box clearly stating, what a user is about to pay with the current settings. E.g. \"In total, SageMath Inc will recurringly charge you X every 1. day of a month.\" \n- [ ] consider switching to using https://github.com/stripe/jquery.payment for input formatting/validation\n- [ ] (1:00?) display charge cards -- button to show more\n\n### DONE\n\n\n## version 0 (before anybody uses)\n\n- [x] (0:32) (0:30?) refactor html of project quota to bootstrap columns\n\n- [x] (3:30+) (1:30?) ui -- get project move with nonfree upgrade implemented\n    - [x] move dialog\n    - [ ] redo with a proper modal dialog that can be used to:\n        - [ ] if select nonfree target and no subscription, ask to confirm upgrade request (checkbox)\n        - [ ] if select nonfree target and no card, ask for card\n\n- [x] (0:51) (0:45?) set default card\n- [x] (0:42) cancel subscription\n- [x] (0:05) (0:45?) #invalid -- seems to be not supported by stripe.  OK. change default billing method\n   - how to set payment method for subscription - by specifying card token using the source param.\n\n\n- [x] (1:00?)  edit billing method -- decided not to; instead user will delete and add new\n- [x] display customer info (so top 10 info about billing, subscriptions, and charge history), and display it all nicely.\n   - [x] (0:22) (0:45?) get customer info whenever billing tab clicked on (put a refresh button too)\n   - [x] (2:10) (1:00?) display charge cards \n   - [x] (0:55) (1:00?) display subscriptions\n   - [x] (0:24) (1:00?) display charges\n\n\n- [x] (0:58) (0:30?) remove billing method\n- [x] (1:20) (0:45?) add new billing method\n- [x] (2:45) (1:30?) create subscription\n\n- [x] (1:00?) list subscriptions\n- [x] (2:00) (0:30?) stripe_create_card\n- [x] (0:29) (0:30?) stripe_delete_card\n- [x] (0:11) (0:30?) stripe_update_card\n- [x] (0:45?) stripe_get_customer\n- [x] (0:15) (0:45?) stripe_get_plans\n- [x] (1:15) (0:45?) stripe_create_subscription\n- [x] (0:25) (0:30?) stripe_cancel_subscription\n- [x] (0:45) (0:30?) stripe_update_subscription\n- [x] (0:21) (0:45?) stripe_get_charges\n\n- [x] (0:17) (1:00?) create new stripe.[coffee/html/css] files and Stripe object, created at right time, etc., with all ui stuff invisible until creation, so can release safely.\n- [x] (0:45) (0:30?) add new subscription\n\n\n- [ ] (3:00?) actually make subscription useful (e.g., move project, etc.):\n   - make we could add a notion of \"resources\" to a project, namely server(s) on which project can be run, which have certain quotas.\n   - for-pay projects will get *option* of running on different resources, which will change their quotas\n   - resource display will show current sync state of everything, load, etc.\n\n","position":-7.444186327746138,"last_edited":1427737596126,"task_id":"7e645381-0a3b-47be-9fc2-273d30916907"}
{"desc":"","position":-6.856888771057129,"last_edited":1425251397117,"task_id":"6877a20e-3766-42f6-b028-0413ff1ecc94","deleted":true}
{"desc":"(0:04) (0:30?) #0 #bug #today\n\nThe new +New button ignores the directory when uploading files...","position":-7.40625,"last_edited":1426826519956,"task_id":"3ad16685-8063-48ee-befd-76aa7874caff","done":1426826519956}
{"desc":"(1:00?) #6 #bug\nlook carefully into whether images get their ttl properly extended on worksheet save for R images.\n\nExample with problems:\n\n   https://cloud.sagemath.com/projects/868087e6-5719-485a-b908-6217b437742c/files/Mathematical%20Statistics.sagews\n   \nRequestor: https://mail.google.com/mail/u/2/#inbox/149dd9a5ff8e5aeb   ","position":-6.856900930404663,"last_edited":1427813589087,"task_id":"ec91c0e1-ecb8-4676-b929-5afafe3cad0d"}
{"desc":"(1:15?) #1 #editor\nimplement copy-from-history button\n\nCould do all this:\n- pop up a dialog that requests filename, defaulting to current filename.\n- have a warning that a file will be overwritten (if it will be)\n\nOr just *do it*, which is probably fine given that it can be undone easily.\n\n- motivated by https://mail.google.com/mail/u/0/#inbox/14be518bc6d7a2ae","position":-6.856901288032532,"last_edited":1427813566858,"task_id":"b540570a-6b76-4b32-92e7-6df8ef9ddb51"}
{"desc":"(3:00?) #1 #com\nimplement account deletion\n\n- needed for commercialization\n- will have to decide where to \"put\" the corresponding projects -- maybe a special DELETED project (?).\n- otherwise probably straightforward.\n\nPeople to email:\n- [ ] https://mail.google.com/mail/u/2/#inbox/14c4603ff576620b\n- [ ] https://mail.google.com/mail/u/1/?ui=2&view=btop&ver=15iwqxmrupss2&search=inbox&th=14c94181bdd49595&cvid=10\n\nI will not throw away data until a waiting period.\n\nAny ideas on semantics?   I'm thinking:\n\n   - set a field in the database saying that the account is deleted\n   - delete password\n   - move any other authentication related info to a different field\n(e.g. email) and remove from the email_address_to_account_id table\n   - remove account_id from all projects user collaborates on\n   - if number of collaborators reaches 0 for any project, add that\nproject to a table of \"orphaned projects\"\n\nSomeday delete orphaned projects.\n\nThis seems like a safe way to have people delete accounts, but it can\nbe recovered from eventually...\n\nHarald says:\n> So, I don't know the database details, but in any case I would argue\nfor a \"grace period\" (correct word?) and be very cautious ... I mean,\nsomething where you just set a field \"deleted\" in the table for the\nusers to the \"current timestamp\" when they want to be deleted, and do\nnothing else, except:\n1. delete all cookies (like the sign-out everywhere does, right?)\n2. when a user who is \"deleted!=None\" tries to log in, give the\nmessage \"your account is deleted, please contact help@...\"\n2.1 if he then tries to sign up, *also* show the \"your account is\ndeleted, please contact help@...\" message.\n3. keep the all collaborators for projects, but do not show those who\nare deleted!=None\n4. if there is ever a way to contact collaborators (or some other way\nto contact users via the SMC platform), do not send messages to those\nwho are marked as deleted.\n5. 4. does also include mass-marketing emails, I saw that you have\nsome code for mass emails\nThis keeps everything in place and an account can be restored easily.\nFor the user, it looks like everything is reset.\nOnce every week, do a query for those who want to be deleted. Since\nthere is this deleted timestamp field, you can query for those who\nare, lets say, 2 weeks old or more, and then start that actual removal\nyou mentioned.\nA remaining issue are backups. IIRC the current TOS do give SMC a\npermission to store the data indefinitely, right? Hence, you do not\nhave to think about changing your backup strategy in order to get rid\nof the deleted project data.     (I'm pretty sure, this wouldn't be the case in the EU, but there are\nother things SMC would have to take care of to operate here)","position":-6.856901466846466,"last_edited":1428709697916,"task_id":"086eecf6-2c66-46c0-ab64-25a6db52be40"}
{"desc":"#6\nimplement realtime messaging between users (e.g., chat)\n\n- Article about google pub/sub: https://news.ycombinator.com/item?id=9145908","position":-7.25,"last_edited":1427813426495,"task_id":"31295a17-1dd7-4396-8073-6f832ac25df0"}
{"desc":"(0:12) (0:15?) #2\nmake smc=salvus as suggested by Jason Grout\n","position":-6.856901556253433,"last_edited":1425621322506,"task_id":"c33a6ed8-a5ec-4c91-b3dc-e9c03f24d23a","done":1425621322087}
{"desc":"(1:00?) #1 #bug #sagews #editor\ncan't insert new line in weird edge case involving ].  \n\nExample here: https://cloud.sagemath.com/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/files/Last%20Bracket%20in%20comment.sagews\n\nFrom this list post: https://groups.google.com/forum/#!topic/sage-cloud/xOm5u6HqnGU","position":-6.85690151154995,"last_edited":1427813522716,"task_id":"63f43b05-ba70-43d2-94bd-30716c7e12d8"}
{"desc":"(1:15?) #sagews #3\nimprove figure sizes in print to pdf\n\nhttps://groups.google.com/forum/#!topic/sage-cloud/NOqwwqJj480\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c1dda214be1da2","position":-6.856901600956917,"last_edited":1427286618879,"task_id":"dfdd5e1f-6971-4ef0-8eba-d093fe2eedcf"}
{"desc":"#feature #5\nmake it so users can easily customize css\n\nhttps://groups.google.com/forum/#!topic/sage-cloud/gyjPXkMm8Lc","position":-6.856901623308659,"last_edited":1426253562375,"task_id":"f217e2b7-2996-4577-bf63-9e3a68741d38"}
{"desc":"(1:15?) #1 #bug #sagews\nblock parsing issue\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14beb5e201d68cf0\n\n    the \"else\" in the following code raises a \"syntax\" error when run in a Sage Cloud cell:\n\n    try:\n        print \"try now\"\n    except:\n        print \"some exception\"\n    else:\n        print \"made it to else\"\n\n    *but* it runs normally if the code is in a function definition or even a junk for loop, like:\n\n    for i in [0]:\n        try:\n            print \"try now\"\n        except:\n            print \"some exception\"\n        else:\n            print \"made it to else\"\n\n    is this a bug, or is there some other reason to explain the difference in behavior?\n\n    thanks,\n\n    craig","position":-6.856901533901691,"last_edited":1426253580823,"task_id":"ac0ebc3f-9128-4cd0-83e1-fae832653864"}
{"desc":"(2:04) (0:45?) #now #bug #0 #today\ntwo people can't use the same terminal session!  \n\nit just keeps resetting\n\nThis turned out to be a basic problem with how sockets were setup/cached between the local and global hub, which wasn't reported by others since it was very unlikely to happen due to multiple hubs.   I also made sure tcp connections are being properly cleaned up, and generally improved reaping when client is destroyed.","position":-6.856902360916138,"last_edited":1425948958335,"task_id":"08389a79-93e8-47e7-859d-93431db27250","done":1425948957930}
{"desc":"(0:45?) #ui #5\nprogress meter when opening any project.","position":-6.856902003288269,"last_edited":1427813467725,"task_id":"e377b067-9435-4518-bdc8-778b47e11e5d"}
{"desc":"(1:00?) #bug #ui #5\nsnapshots/backups directory should *always* be displayed in date order, no matter what user pref is.","position":-7.3125,"last_edited":1427813418063,"task_id":"f99230e3-df9e-4913-9ad7-14f32a4f827b"}
{"desc":"(0:15?) #help #ui #1\nlink to new youtube video somewhere in help page\n\nhttp://youtu.be/_ff2HdME8MI","position":-7.34375,"last_edited":1426252984403,"task_id":"3da76dd0-c66a-45f2-bb48-335db09e2701","done":1426252983996}
{"desc":"(1:30?) #ipython #5\nprint to pdf via nbconvert\n\nBasically, use `ipython nbconvert --to latex` \n\nFixed: https://mail.google.com/mail/u/0/#inbox/14c05565b8f47040","position":-7.328125,"last_edited":1427206697196,"task_id":"420c1a11-b362-4f18-856d-8d4b7797f206"}
{"desc":"(0:16) (0:15?) #bug #1 #today\n\nwhen copying files between projects.\n\n\"Successfully copied support/2015-03-11-093745-axiom-integral.sagews to support/2015-03-11-093745-axiom-integral.sagews in undefined\"","position":-7.4453125,"last_edited":1427257159757,"task_id":"936146f2-b93f-48ba-8856-dfb14074396f","done":1427257159353}
{"desc":"(0:23) (0:30?) #0 #bug #public #today\nsharing a whole project publicly doesn't seem to make the files in it public, though they are listed as such","position":-7.453125,"last_edited":1427078334355,"task_id":"0f7c8613-01d7-43ae-aba5-9e1ab4004d65","done":1427078333943}
{"desc":"(5:00?) #translate #3\nMake SMC have UI language options for:\n\n  - turkish\n  - russian\n  - English\n  \nUse the code from https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c07f4ded3564fe  ","position":-7.326171875,"last_edited":1428069871634,"task_id":"6486c0e1-c56a-45dd-9e43-d76548f76548"}
{"desc":"(3:00?) #1 #ui #bug\nProper file rename and move\n\nE.g., https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c0e83cfe2804fe","position":-7.444186325650662,"last_edited":1427813355713,"task_id":"6cb2acb3-019e-4046-b8d5-1aaa782a46ac"}
{"desc":"(0:30?) #bug #sagews\nsagews cython issues\n\n- [ ] clicking on cython code link doesn't pop it up (link is right though)\n\n- [ ] Also, there is a deprecation warning:\n\n        /projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/.sagemathcloud/sage_server.py:920: DeprecationWarning: \n        Importing tmp_dir from here is deprecated. If you need to use it, please import it directly from sage.misc.temporary_file\n        See http://trac.sagemath.org/17460 for details.\n          code = code_decorator(code)","position":-7.444186321459711,"last_edited":1427838569424,"task_id":"d5453394-1eb3-4246-b0e8-afe114edaca9"}
{"desc":"(2:27+) #today #now (2:00?) #upgrade #ipython #1\nipython3 -- notebook version 4\n\n- [x] (4:00) disconnect and change each; totally breaks sync. Took a long time; fixing major and subtle sync issues...\n- [x] (0:30) ipython upgrade breaks command line sage: `ImportError: cannot import name warn_format_error`:  FIX: https://github.com/sagemathinc/smc-sage/commit/6818fb80e2bf6a61c38b1980e8a91f61a101768e\n\n- [x] (0:29)periodically watch for changes in the actual .ipynb file -- maybe every 5-10 seconds? NO -- too hard for now.   But at least make sure that changing file and opening works.`\n\n- [x] (0:14) notification history and syncdoc file formats...\n- [x] %load_ext sage\n- [x] (5:00) history slider\n\n\n\n\n\n\n\n- [x] (0:36) R kernel support: (follow https://github.com/IRkernel/IRkernel)\n        \n- [x] FAIL (1:04) #new Haskell kernel support - the ubuntu-install script fails to build some graphical tools (hence semicolon below). This build line takes about an hour!?\n\n\n- [x] (1:10) other kernels (e.g., julia), ipython3:\n\n   \t\tsudo pip3 install --upgrade ipython\n        sudo ipython3 kernelspec install-self\n        \n        Then edit /usr/local/share/jupyter/kernels/python3 and add a \"-E\" option in so that python3 can\n        start with the sage -sh environment set.\n        \n- ijulia support:\n\n        umask 022\n        sudo su\n        export JULIA_PKGDIR=/usr/local/share/julia/site/\n        julia\n        julia> Pkg.init()\n        julia> Pkg.add(\"IJulia\")\n \t\tcp -rv \"/root/.sage/ipython-2.3.0.p0/kernels/julia 0.3\" \"/usr/local/share/jupyter/kernels/julia 0.3\"\n        \n        {\n          \"display_name\": \"Julia 0.3.7\",\n          \"argv\": [\n            \"/usr/bin/julia\",\n            \"-i\",\n            \"-F\",\n            \"/usr/local/share/julia/site/v0.3/IJulia/src/kernel.jl\",\n            \"{connection_file}\"\n          ],\n          \"language\": \"julia\"\n        }\n\n\n---\n\nhttps://mail.google.com/mail/u/2/#inbox/14c229fd5f9407ea\n\nIdeas: https://mail.google.com/mail/u/2/#inbox/14c229fd5f9407ea\n\nformat version 4.\n\n    The attached IPython Notebook has the format version 4. This is the\n    default for IPython 3 and upwards. SMC runs IPython 2.3.x, which is\n    too old to understand the version 4 format ... only version 3.\n    Therefore, you have to convert your notebook to v3 before running it\n    on SMC.\n\n    You can do this in the terminal via:\n\n    ipython nbconvert --nbformat=3 --to notebook \"Fails-Answers to Actual\n    Exam 3.ipynb\"\n    \n    \nDoing\n\n- [x] `umask 022` then `sage -sh` then `pip install --upgrade ipython jsonschema mistune`\n- [x] change the ipython notebook logo to jupyter...?\n- [x] testing with this disabled: `websocket_reconnect`\n- [x] disable autosave\n- [x] refactor code out\n- [x] nbviewer stuff: test that public display works and ALSO producing the thing.\n- [x] (1:33) change so instead of /port/port-number one gets the ipython notebook server via /port/jupyter.  This would fix the reconnect issue on server restart/move, simplify code, etc.  It's much, much more logical. \n- [x] (0:23) load jupyter static resources through nginx for much better speed... Change `/usr/local/sage/current/local/lib/python/site-packages/IPython/html/notebookapp.py` as follows (then type `/usr/local/bin/ipython notebook`):\n\n\t\t#static_url_prefix = url_path_join(base_url,'/static/'),\n        static_url_prefix = url_path_join('/static/jupyter/'),\n","position":-7.444186296314001,"last_edited":1428355515395,"task_id":"4983adf9-0549-426c-a9fe-a18281aa9f20","done":1428355514992}
{"desc":"(1:00?) #bug #md\nmarkdown printing title issue\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c1e1e28cd397b7","position":-6.856901578605175,"last_edited":1427813490605,"task_id":"cb997db0-a22d-43b9-95ee-d58be8633208"}
{"desc":"#bug\nweird PATH issues:\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c1ad72958f4d28\n\nWhen I do the following I see repeated entries in the PATH variable.\n\n        Ralf\n\n        ~$ for l in $(echo $PATH |sed \"s|$HOME|\\$HOME|g;s/:/ /g\"); do echo $l; done\n        $HOME/.sagemathcloud\n        $HOME/.sagemathcloud\n        $HOME/.sagemathcloud/node_modules/coffee-script/bin/\n        $HOME/.sagemathcloud/node_modules/.bin/\n        $HOME/.sagemathcloud/data/local/bin\n        $HOME/bin\n        $HOME/.sagemathcloud\n        $HOME/.sagemathcloud/node_modules/coffee-script/bin/\n        $HOME/.sagemathcloud/node_modules/.bin/\n        $HOME/.sagemathcloud/data/local/bin\n        $HOME/bin\n        $HOME/.sagemathcloud\n        $HOME/.sagemathcloud/node_modules/coffee-script/bin/\n        $HOME/.sagemathcloud/node_modules/.bin/\n        $HOME/.sagemathcloud/data/local/bin\n        $HOME/bin\n        $HOME/.sagemathcloud\n        $HOME/.sagemathcloud/node_modules/coffee-script/bin/\n        $HOME/.sagemathcloud/node_modules/.bin/\n        $HOME/.sagemathcloud/data/local/bin\n        $HOME/bin\n        /usr/local/bin\n        /usr/bin\n        /bin\n        /usr/local/games\n        /usr/games","position":-6.856901567429304,"last_edited":1426550644045,"task_id":"a3519946-a72f-4ecc-bfc9-71b362efaec5"}
{"desc":"(1:00?) #bug #5\n`bup_storage.py stop` needs to account for lock files, both for user and group\n\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# bup_storage.py --zpool bup stop 509eed88-d82a-4cb8-a66b-b93681b96a3e\n    stop(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}):\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}):\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}): killing all processes by user with id 1860766618\n    /usr/bin/killall -u 509eed88d82a4cb8a66bb93681b96a3e\n    /usr/bin/pkill -u 1860766618\n    /usr/bin/killall -9 -u 509eed88d82a4cb8a66bb93681b96a3e\n    /usr/bin/pkill -9 -u 1860766618\n    pgrep -u 1860766618\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}): kill attempt left 0 procs\n    /usr/sbin/userdel 509eed88d82a4cb8a66bb93681b96a3e\n    /usr/sbin/groupdel 509eed88d82a4cb8a66bb93681b96a3e\n    zfs set userquota@1860766618=none bup/projects\n    (0.00465106964111 seconds):\n    zfs set userquota@1860766618=none bup/scratch\n    (0.0046169757843 seconds):\n    fusermount -uz /projects/509eed88-d82a-4cb8-a66b-b93681b96a3e/.snapshots\n    total time: 0.589437007904 seconds\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# vi /etc/passwd\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# vi /etc/passwd\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# /usr/sbin/userdel 509eed88d82a4cb8a66bb93681b96a3e\n    userdel: existing lock file /etc/subuid.lock without a PID\n    userdel: cannot lock /etc/subuid; try again later.\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# more /etc/subuid.lock\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# rm /etc/subuid.lock\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# /usr/sbin/userdel 509eed88d82a4cb8a66bb93681b96a3e\n    userdel: existing lock file /etc/subgid.lock without a PID\n    userdel: cannot lock /etc/subgid; try again later.\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# ls -lht /etc/subgid.lock\n    -rw------- 1 root root 0 Feb 25 22:43 /etc/subgid.lock\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# rm /etc/subgid.lock\n    root@compute4dc1:/projects/509eed88-d82a-4cb8-a66b-b93681b96a3e# bup_storage.py --zpool bup stop 509eed88-d82a-4cb8-a66b-b93681b96a3e\n    stop(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}):\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}):\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}): killing all processes by user with id 1860766618\n    /usr/bin/killall -u 509eed88d82a4cb8a66bb93681b96a3e\n    /usr/bin/pkill -u 1860766618\n    /usr/bin/killall -9 -u 509eed88d82a4cb8a66bb93681b96a3e\n    /usr/bin/pkill -9 -u 1860766618\n    pgrep -u 1860766618\n    killall(project_id=509eed88-d82a-4cb8-a66b-b93681b96a3e,{}): kill attempt left 0 procs\n    /usr/sbin/userdel 509eed88d82a4cb8a66bb93681b96a3e\n    (0.119844913483 seconds):\n    /usr/sbin/groupdel 509eed88d82a4cb8a66bb93681b96a3e\n    zfs set userquota@1860766618=none bup/projects\n    (0.00463485717773 seconds):\n    zfs set userquota@1860766618=none bup/scratch\n    (0.00438094139099 seconds):\n    fusermount -uz /projects/509eed88-d82a-4cb8-a66b-b93681b96a3e/.snapshots\n    total time: 0.684422969818 seconds","position":-6.856901561841369,"last_edited":1427813502174,"task_id":"3790e8f6-d11e-4ddd-aa81-fcf4ce83ecc1"}
{"desc":"(1:30?) #1 #bug\nrewrite ipython daemon startup to work 100% even on load -- not possible to DOS machine.  Probably need a lock file.","position":-7.37890625,"last_edited":1428504016380,"task_id":"b029ef51-09d8-4ebb-ac29-5d4720ed44f3","done":1428504015964}
{"desc":"(0:28) (0:45?) #1 #bug #today\nkeyboard shortcuts for tasks conflict with searching in the notification box.","position":-7.421875,"last_edited":1426826247034,"task_id":"6ac35ebe-a31d-4df2-ba04-d981ff10fe66","done":1426826246628}
{"desc":"(0:45) (1:00?) #1 #today #now\nif `bup_storage.py` fails which results in an error (e.g., traceback when doing status), it should run stop first on that project and try again.  This is important to fix!","position":-7.3828125,"last_edited":1427082144533,"task_id":"79d6cbac-cee4-4cfb-bbe4-620fd20378e6","done":1427082144129}
{"desc":"#install\nmayavi install help\n\nhttps://mail.google.com/mail/u/0/#inbox/14c263be9a282d63","position":-7.367431640625,"last_edited":1427206529905,"task_id":"e825f9a1-da88-4506-a8f9-72fc79db9792"}
{"desc":"(0:30?) #5\nmathbook xml demo \"private beta\"\n\nhttps://mail.google.com/mail/u/0/#inbox/14c25eddde6bd54d","position":-7.444186313077807,"last_edited":1428878865151,"task_id":"95140ab4-e199-4478-9af7-b037ef472ec0","done":1428878864730}
{"desc":"#3\nrequest to build root library from source differently.\n\nhttps://mail.google.com/mail/u/2/#inbox/14c290ca76dcce7c","position":-6.856901559047401,"last_edited":1428069851273,"task_id":"c718ee4d-bf78-4fe4-8f63-a2cac704b504"}
{"desc":"(2:25) #now (1:30?) #1 #today\nrudimentary handling of zip files.","position":-7.46875,"last_edited":1427070747807,"task_id":"17898fb5-8eba-4a87-9446-606ba2a81412","done":1427070747394}
{"desc":"(1:30?) #4\nmake it possible to set the default login shell for a project\n\nSee https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c3e57d74c6b183\n\nProbably want to make this part of project configuration, then add to bup_storage.py, etc.","position":-7.373046875,"last_edited":1427812942615,"task_id":"611cd679-8b43-47cf-8474-328e841fab45"}
{"desc":"(0:45?) #2 #speed #unclear\nthe `git-ls` command can be slow; maybe just parse ls (?)","position":1.90625,"last_edited":1427813741555,"task_id":"2041e17c-8274-4d1a-80e9-c4f41ff461ba"}
{"desc":"(0:27) (0:20?) #0 #today #bug\ndisable forever loging for hub, which wastes space","position":-7.4296875,"last_edited":1427080218559,"task_id":"98fbbd4d-01a8-4dc5-92c1-9ccd6ad88e3c","done":1427080218145}
{"desc":"(1:28) (0:30?) archive #today\n- [ ] support .tar.bz2, .tar.gz, .tar","position":-7.43359375,"last_edited":1427088117792,"task_id":"66c7cf38-4929-4308-9fba-e3b44b787ad7","done":1427088117390}
{"desc":"(0:24) change python used for the scripts in ~/.sagemathcloud, to make things more robust in case user messes up their python install.","position":-7.435546875,"last_edited":1427125424437,"task_id":"a99823d6-387c-4f78-a6db-b073b6a8ce4a","done":1427125424026}
{"desc":"(0:42) (1:00?) #today #archive\nmake it so extracting archive will optionally (by default) create a directory if none","position":-7.376953125,"last_edited":1427128056523,"task_id":"d94d9b9d-7bcc-4183-a8f2-43da159a9eff","done":1427128056113}
{"desc":"(2:00?) #ui #1\nre-implement file drag and drop, at least if number of files isn't too big (?)","position":-7.3759765625,"last_edited":1427812924432,"task_id":"359ec701-68f7-46fd-aabd-d1dd1b65dac2"}
{"desc":"(0:30?) #archive #mobile #bug #4\nmake archive mobile-friendly with a close button","position":-7.37548828125,"last_edited":1427812931240,"task_id":"e9d11398-dc1b-47c6-999d-e8f0044fbacb"}
{"desc":"(1:00?) #2 #upgrade\nupgrade codemirror 5.2 -- https://mail.google.com/mail/u/0/?pli=1#inbox/14cd668f67334443","position":-6.856901433318853,"last_edited":1429540628641,"task_id":"8bde9fe0-32a3-4389-b798-a62d3a2a9e8b"}
{"desc":"(6:00) (1:30?) #sagews #now\nimplement `raw_input` in sagews\n\n(ended up taking way longer than I thought, and using a completely different approach.  Resulted \nin rewriting some things much more efficiently involving rendering worksheets, which might fix some important bugs.)\n\nreplace the built-in raw_input function by a special SMC one that:\n\n- [ ] sage server: new output type which is called raw_input; implement in server and client\n\n\t     {raw_input:{prompt:'input stuff?', value:'', submitted:false}}\n\n- [ ] as user types, change the document to reflect their input so far; then everybody will see. \n         \n\t     {raw_input:{prompt:'input stuff?', value:'william', submitted:true}}\n         \nOnce somebody sets done=true, all prompts change to have a checkbox or something and be read-only.         \n         \n- [ ] in the `raw_input` python function running in server, after sending the raw_input output, switch to a mode where it waits for exactly one raw_input message in response from client.\n\n- [ ] When done:true for a raw_input when sage server in waiting state,  then local_hub sends message to sage_server containing the value.\n\n- [ ] In raw_input, handle that one raw_input message and move on.","position":-6.856813311576843,"last_edited":1427737349416,"task_id":"eb2b926e-62d4-4879-9972-2c8d76e1333a","done":1427737349013}
{"desc":"(0:15) (0:30?) #install  #today\nfricas as a new Sage optional package\n\nhttps://mail.google.com/mail/u/2/#inbox/14c0e22836d73fef","position":-7.3271484375,"last_edited":1427257177858,"task_id":"ca4727da-86eb-487e-a23a-66f7c8b4c5d2","done":1427257177452}
{"desc":"(0:30?) #sagews #bug\nimplement printing `raw_input`","position":-7.32666015625,"last_edited":1427208196832,"task_id":"7c7f9cce-0f17-4997-8c73-a99d8a3b8a05"}
{"desc":"(1:30?) #sagews #bug\n`raw_input` inside an interact -- need to implement support for clear and delete_last_output, i.e., for the output message series changing...","position":-7.326416015625,"last_edited":1427208178583,"task_id":"bc21522b-116f-4c5d-a2d1-1a6cc0af4892"}
{"desc":"(1:10+) (2:00?) #1 #today\nmake this url display copyright statement, read from a static file (?)\n\n- [ ] https://cloud.sagemath.com/articles/copyright\n- [ ] https://cloud.sagemath.com/articles/terms\n- [ ] https://cloud.sagemath.com/articles/pricing\n- [ ] https://cloud.sagemath.com/articles/privacy\n\nBasically make https://cloud.sagemath.com/articles a static nginx-served site. Also link to each item from within SMC.\n\nAsk if a tl;dr; is dangerous -- https://mail.google.com/mail/u/2/#inbox/14c4cd26fbc6731e","position":-6.856902718544006,"last_edited":1427242736142,"task_id":"5c9974a6-d1df-41f7-8792-111f1533295a","done":1427242735725}
{"desc":"(1:00?) #1 #today\nmake this url display fees, read from a static file (?)\n\n  https://cloud.sagemath.com/articles/pricing","position":-6.856902539730072,"last_edited":1427215962170,"task_id":"042b0739-6a41-49c5-81c8-f8eef1385334","deleted":true}
{"desc":"(0:43) (0:30?) #today\n\n- [ ] make new terms of service live for new accounts and from help page.","position":-7.326904296875,"last_edited":1427245416939,"task_id":"bb1342fb-dbb2-45ea-ae0f-9e5936436869","done":1427245416534}
{"desc":"(0:30?) #latex #feature\nblock comment/uncomment button","position":-7.32763671875,"last_edited":1427285826616,"task_id":"8c7d4c87-ce79-4746-9a89-7583c4ddcf74","deleted":true}
{"desc":"(2:30?) #latex #2\nmake the new latex editing bar live for the latex editor itself.\n\n- [ ] fix the comment/uncomment button to properly work for multiple lines\n\nEmail this when done: https://mail.google.com/mail/u/0/#inbox/14c50d587e1469e6","position":-7.327880859375,"last_edited":1427813366984,"task_id":"4d61c0e4-9d18-46b9-b166-2b199f7a8608"}
{"desc":"(0:30?) #ui #4\nproject tabs should have a tooltip showing the full project name (like file tabs)","position":-7.3277587890625,"last_edited":1427286235627,"task_id":"30bcbfa4-d10e-4857-9847-4fbfbcea0b8e"}
{"desc":"(4:00?) #2 #today\nimplement a first basic single sign on -- will help with growth and commercialization\n\n- [x] (4:20) get a complete basic demo using passport to work; this took forever because of realizing I should rewrite the web server to use express (done), then running into a lot of problems due to changes in Express 4 versus previous versions of Express, which led to silent failure.  Figured out solutions by printing print statements in the passport source code...\n\n- [x] (2:08) (1:00?)  basic demo test using one of google/facebook/github/wordpress (?) or something:\n    - spent about 20m dealing with my own firewall blocking the google oauth server.  Fixed.\n    - next problem: the recommended library passport-google uses openid2, which is deprecated in a few days!   So instead, I have to use oauth2, which is in https://github.com/jaredhanson/passport-google-oauth, which I found by luck!\n    - and many more problems... documented internally. \n- [x] (0:30) (0:45?) similar demo as above, but for github.\n- [x] (0:13) (0:20?) passport for facebook\n- [x] (0:20) (0:20?) passport for dropbox\n- [x] (0:29) (0:20?) passport for bitbucket -- harder due to lack of oauth2...\n- [x] (0:21) (0:20?) passport for wordpress\n- [x] (0:13) (0:20?) passport for twitter\n\n\nJust spent 2:30+ figuring out how these things work, but not implementing anything.  And planning.\n\nIdeas:\n- When creating an account, have a button for username/password, facebook, github, google, and wordpress (for now).\n- When auth'd, record info about user in accounts table for that auth method, so have\n  - fields: 'auth_google', 'auth_local', 'auth_facebook', 'auth_github', 'auth_wordpress', \n  - value: {'id':..., etc.}\n- Move existing password hash field to `auth_local`.\n- Redo the table `email_address_to_account_id` to be: `passport_token_to_account_id`, which is a map:\n        provider-id --> account_id\n  so we make the key the string got by combining the provider and the id, with a dash.\n  This way we can have multiple authentication methods for a user.\n  Will have to change all existing stuff to `local-email_address`.  \n- Add new field to accounts which is called \"search_name\", which replaces existing ones.  The search_name can combine together all name fields, so only have to query that one thing when doing full text search.  Whenever we update auth stuff for a user, also update the search_name field. \n- Extend user search to allow restricting to users that have linked their profile with Google/Github/etc. and do that search separately.\n\nEmails to respond to:\n- https://mail.google.com/mail/u/0/#inbox/149c8ee81aaca138   \n- https://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14bc1d38c071e9ff\n\n\nOn servers to set this up, will need to:\n\n    npm install --upgrade passport\n    npm install passport-dropbox-oauth2\n    npm install passport-facebook\n    npm install passport-github\n    npm install passport-google-oauth\n    npm install passport-local\n    npm install passport-wordpress\n    npm install passport-twitter\n    npm install passport-bitbucket\n    npm install express-session\n    npm install body-parser\n    \nIn Cassandra:\n\n\n    CREATE TABLE passport_settings (\n        strategy varchar PRIMARY KEY,\n        conf     map<varchar:varchar>\n    );\n\nExample of entering some *testing* credentials:\n\n    update passport_settings set conf['auth']='https://william.wstein.org/auth' where strategy='site_conf';\n\n\tupdate passport_settings set conf['clientID']='...' where strategy='google';\n    update passport_settings set conf['clientSecret']='...' where strategy='google';\n    ","position":-7.4405517578125,"last_edited":1427550694181,"task_id":"06ebb41c-cf63-4c65-82f4-8759d315cd9d","done":1427550693775}
{"desc":"(0:07) (0:30?) #today\nmerge in harald's new terms of service\n","position":-7.4365234375,"last_edited":1427289165025,"task_id":"0936c228-3dd0-4b4c-ab7c-5aa2b577f483","done":1427289164612}
{"desc":"(0:30?) #1 #bup #bug #security\nsparse files -- if the user creates a massive sparse file (not in an excluded directory), the bup save process will spin at full cpu forever, basically DOS'ing that computer.  NOT GOOD.\n\n**Solution:** Make use of this bup option!\n\n      --smaller=maxsize\n              don't  back  up files >= maxsize bytes.  You can use this to run\n              frequent incremental backups of  your  small  files,  which  can\n              usually  be  backed  up  quickly, and skip over large ones (like\n              virtual machine images) which take longer.  Then you can back up\n              the  large  files less frequently.  Use a suffix like k, M, or G\n              to  specify   multiples   of   1024,   10241024,   10241024*1024\n              respectively.","position":-7.444186262786388,"last_edited":1428102562298,"task_id":"b57c8e5e-286b-4ca0-b951-d5ec80480f7c"}
{"desc":"#9 #feature #editor\n\nconsider incorporating http://webodf.org/demos/ into SMC, so we can view/edit odf and docx files better.","position":-7.4404296875,"last_edited":1427812838386,"task_id":"089c9fca-924b-496a-9843-635e957e8545"}
{"desc":"(2:25) (2:00?) #today #now #hub\nrewrite hub webserver to use express, so that I can use passport for auth.\n\n- [ ] ** read basics of express\n- [ ] try to migrate simple url parameters\n- [ ] deal with tricky cookies stuff carefully!","position":-7.44091796875,"last_edited":1427332395930,"task_id":"44241df0-64e7-4e36-8f8c-0b846d1fa802","done":1427332395516}
{"desc":"(0:30?) #1 #security #critical\ndelete password immediately after submitting for login\n\n- it seems like maybe on first loging (or account creation?) the DOM still has the user password in it.  That's a security risk and serves no purpose.\n\n","position":-7.440673828125,"last_edited":1427810863757,"task_id":"6125e816-d712-4ade-9882-cc9dd7da645d"}
{"desc":"(1:15?) #optimize #7\nadd an optional in-memory cache for key:value store so don't hit database every time certain queries like this in hub happen:\n\n    database.key_value_store(name:'global_admin_settings').get","position":1.935546875,"last_edited":1427327932804,"task_id":"b0fdaea3-da40-4100-acaa-5b7b718ac343"}
{"desc":"(0:35) (1:00?) #0  #bug #today\nmore usage data link is broken for people who don't own the stats project!  it downloads the file rather than serving it...\n\nThis probably represents a very serious pub in making raw stuff public with the headers.\n\nSimilar bug report about pdf's","position":-7.44415568956174,"last_edited":1427760878789,"task_id":"566f8391-f871-436e-b108-6c782fb6e8e3","done":1427760878382}
{"desc":"","position":-7.3280029296875,"last_edited":1427392026063,"task_id":"d2be34b0-a5ce-45ca-8ea9-3e8ca9de2166","deleted":true}
{"desc":"(0:45?) #bug #tasks #7\ndue date not in red when on the *same* day.  \"E.g., about 21 hours ago\"","position":-7.32781982421875,"last_edited":1427392771559,"task_id":"ba881e23-9292-4e28-bf0f-019608ba0367"}
{"desc":"(1:30?) #5 #com\nidle timeout status. \n\nhave an api and ui that shows how long until the project idle timeouts.   User will see which actions trigger increasing the timer.  Have link to upgrade project right there.  Part of the pitch, but also clarifies things. Critical.\n","position":-7.4423828125,"last_edited":1427812807941,"task_id":"b813180b-f626-4be6-aa3e-2a9a53964990"}
{"desc":"#idea\ncodenvy -- development ideas\n\n - https://codenvy.uservoice.com/forums/183121-general\n - request for feedback: https://mail.google.com/mail/u/0/?pli=1#sent","position":1.93359375,"last_edited":1427812690072,"task_id":"a0b43f77-a039-4670-aa4c-3834b7945ba4"}
{"desc":"(days?) #7\nimplement oauth2 server developer support\n\nSee this email for how -- https://mail.google.com/mail/u/2/#search/api/14c532da99c66c3a\n\nIn particular, will use this library: https://www.npmjs.com/package/node-oauth2-server","position":-7.444091796875,"last_edited":1427812441408,"task_id":"cbe4c7b1-6ade-4db2-9a48-d4c2cd22ca4f"}
{"desc":"#today #passport\n\n- design and make this work properly (including adding new connections), and implement\n\nWhat's the plan?  So, I can now do the oauth dance for google, github, facebook, dropbox, bitbucket, wordpress, twitter.  There are two things to do:\n- allow account creation by clicking on a button for one of those sites; will need a url that returns the *supported* passport sites, e.g., auth/sites; then the client ui can display buttons for them during signup\n- allow linking an existing account with any of these sites.\n- allow disabling local password-based login\n- LATER: use linking for something besides login in some cases.\n- I think facebook/twitter don't have email addresses; that only impacts searching.\n\n## PLAN:\n\n- [x] (0:15) (0:30?) implement auth/strategies url\n- [x] (0:47) (1:00?) implement showing buttons during account creation based on auth/sites\n- [x] (3:49) (2:00?) #now implement account creation via passport for each strategy; will involve making some decisions about how to store profile info in database, dealing with route handling, etc.\n\n        CREATE TABLE passports (\n            id         varchar,\n            strategy   varchar,\n            account_id uuid,\n            profile    varchar,   /* raw user profile that we got from the authentication provider */\n            activated  boolean,\n            PRIMARY KEY(id, strategy)  /* id is first so it is the cluster key, so data more smoothly distributed, rather than grouped by strategy */\n        );\n        \n        alter table accounts add  passports map<varchar, varchar>;\n\n- [x] critical point: we don't want to *trust* the oauth providers to provide valid emails.  E.g., if my account is as wstein@example.com, and some random joe (say at github!) makes a github account with that email by hacking their system, they shouldn't get access to my account.  So when we add a passport, if there is already a login/password account, we should require authentication again before activing the passport.\n\n- [x] (1:30) (2:00?) implement login via passport:\n    - if passport is activated, set remember_me for cookie and login\n- [ ] change the     \n- [ ] (1:00?) in account settings, clicking on button to add linked account\n- [ ] (0:45?) click on button to unlink a linked account.\n- [ ] #invalid (1:00?) If passport is not activated, display a form requesting their password (showing the user profile!) and include a token in it; when submitted it will activate that profile.  \n- [ ] things like \"change email\" that require a password can't require one if passports are *only* auths...\n- [x] Loading 'Undefined\" (spinner) -- should be just Loading...?\n- [x] (1:00?) implement in account settings showing what o-auth strategies are available/setup\n- [ ] (1:00?) let user connect their account with new authentication mechanisms.\n- [x] (0:30?) deal with oauth1 secret issue.\n- [ ] make terms of service purely client side\n- [ ] oauth + registration token --deal with this somehow...\n\n## (Much) LATER?...\n- [ ] (3:00?) enhance collab search with checkboxes to restrict to users who have linked their acocunt using an auth/site","position":-7.4442138671875,"last_edited":1427550377329,"task_id":"ba20b267-791d-4a63-a80e-6c4bc38f4dcf","done":1427550376913}
{"desc":"(2:00?) #1 #com #security\nintroduce a \"number of projects\" quota, since it would be easy to DOS SMC by creating lots of projects.\n\nMake it possible to raise quota via an email request.","position":-7.44415283203125,"last_edited":1428069739509,"task_id":"2069a40e-209b-46bd-aaa9-d0334d973048"}
{"desc":"#now (1:24) (1:00?) #today #passport\npassport: make login use the remember_me cookie to associate with an account (if remember_me is available), rather than using an email search (it will still do search, but instead as warning, and have an option)","position":-7.444188117980957,"last_edited":1427562329325,"task_id":"95fd14cf-751d-482c-8731-fd9d20c7d29d","done":1427562328921}
{"desc":"(1:00?) #today #passport\nif email is already in use, given user the choice of creating new account, or login with that email instead, then link","position":-7.444185972213745,"last_edited":1427583049180,"task_id":"9eedddae-eb7b-4dff-9e69-c727dd3b76e9","done":1427583048772}
{"desc":"(1:00?) #today #passport\nin account settings, when clicking on passport button that isn't already configured, pop up dialog saying \"This will link...\", then actually do it (by popping up separate window to complete process).","position":-7.444160461425781,"last_edited":1427582323636,"task_id":"3185ee57-f0f8-486e-a677-7b9c368bd6b4","done":1427582323230}
{"desc":"(0:45?) #today #passport\nin account settings, when clicking on passport button that is configured, pop up dialog saying \"This will unlink...\".  Implement that backend message to do unlinking.","position":-7.444156646728516,"last_edited":1427582326260,"task_id":"3c709fbe-55f7-4f9f-be13-5a979a817a06","done":1427582325845}
{"desc":"(0:04) (0:30?) #today #passport\ncompletely remove anything related to \"activated\" and passports from the backend code/database setup -- bad idea.","position":-7.4441986083984375,"last_edited":1427551769930,"task_id":"72c8bb47-92b7-4177-b872-20fa19998ad7","done":1427551769528}
{"desc":"#now (0:53) (0:30?) #today #passport\nterms of service requirement when doing passport account creation","position":-7.444190979003906,"last_edited":1427555052185,"task_id":"25e8c271-13bc-4daf-bd36-8e0c71bef65b","done":1427555051778}
{"desc":"(2:25) (1:15?) #now #today #passport\npassport: changing password and change email in account settings need to be aware that user might not have a password/email.\n\n- [ ] (0:45?) setting email in first place, require setting a new password (not getting an old one)\n- [ ] (0:30?) can't set password if no email\n\ntook longer since I ended up having to refactor and revamp a lot of code...","position":-7.444186687469482,"last_edited":1427570857224,"task_id":"f02083e3-5512-4542-badd-11b411865e94","done":1427570856824}
{"desc":"(1:41) (1:30?) #today #passport\nmake it so there are two types of logout: (1) normal sign out on a single client, and (2) sign out everywhere, which invalidates all remember me cookies for all clients, even for oauth-based logins.\n\n- [x] (0:30?) add the remember_me cookies made in passport_login to the accounts table properly.\n- [x] (0:30?) add ui stuff to have two versions of sign out (and move button to the right); put a conf for the second sign out\n- [x] (0:30?) backend messaging for each type of sign out.","position":-7.444186329841614,"last_edited":1427590006092,"task_id":"1511a5d2-115e-492f-aabb-03d0392c7433","done":1427590005681}
{"desc":"(2:30?) #0 #passport\ndeploy passport stuff live\n\n- [x] (0:05?) Create database schemas:\n\n\n        CREATE TABLE passport_settings (\n            strategy varchar PRIMARY KEY,\n            conf     map<varchar, varchar>\n        );\n        \n        CREATE TABLE passports (\n            id         varchar,\n            strategy   varchar,\n            account_id uuid,\n            profile    varchar,   /* raw user profile that we got from the authentication provider */\n            PRIMARY KEY(id, strategy)  /* id is first so it is the cluster key, so data more smoothly distributed, rather than grouped by strategy */\n        );\n        \n        alter table accounts add  passports map<varchar, varchar>;\n        \n- [x] (0:20?) install npm dependencies\n\n        npm install --upgrade passport\n        npm install passport-dropbox-oauth2\n        npm install passport-facebook\n        npm install passport-github\n        npm install passport-google-oauth\n        npm install passport-local\n        npm install passport-wordpress\n        npm install passport-twitter\n        npm install passport-bitbucket\n        npm install express-session\n        npm install body-parser\n    \n\n- [x] (0:05?) make sure to do ./update_version for a commit.\n\n- [x] (0:30?) push code to vm's, restart servers, test\n\n","position":-7.444156169891357,"last_edited":1427722958950,"task_id":"a6b4ebe5-78fb-45fc-b30f-67db04d21085","done":1427722958537}
{"desc":"(0:35+) (2:00?) #1 #passport\nmultiple email addresses per account, with email address verification and do the account creation actions only on email address verification.\n\n- [ ] (0:15?) consider oauth-provided emails as verified\n- [ ] (0:30?) ui button and api calls to send verify email message for non-verified addresses\n- [ ] (0:30?) automatically send verify email whenever email addresses added/changed\n- [ ] (0:45?) handling code for verification link clicking and then doing account creation actions for that email (and marking them done); don't do \"account creation accounts\" until verify happens.\n","position":-7.444155693054199,"last_edited":1427810840855,"task_id":"a6ad7d75-a5a3-4312-878b-dc505165f92e"}
{"desc":"(1:00?)  #passport #1 #security\nrequire secret token, even for social sign up\n\n(could do by setting a random 1-time cookie in browser that's also in db)","position":-7.444153308868408,"last_edited":1428070078476,"task_id":"03138356-45f3-44a9-a450-517591d86101"}
{"desc":"#accounts #feature #unclear #9\nsupport login via multiple accounts at once (?)\n\n- [ ] change the remember_me cookie name to also include the account_id?\n- [ ] have remember_me cookie itself contain the account id's?\n- [ ] in url have a ?account_id= option... or put it in the URL (ugh).   don't know.\n\nNOT SURE if this is a good idea or what. Google's way sucks.","position":1.921875,"last_edited":1427812284688,"task_id":"d92e28db-a2cf-48c6-a85a-8621f57f1bc4","done":0}
{"desc":"(3:00?) #today\nshutdown all compute vm's at dc2\n\nI have no good code for eliminating a machine!\n\n- [x] (0:25) write such code.\n- [x] (0:15) also write backup code\n- [x] create backup and spot check\n- [x] (2:00?) debug/test decommission code, and run...\n","position":-7.4441550970077515,"last_edited":1427694027439,"task_id":"fbefe5c3-d841-4a8b-87e7-e3c9f4f2fdb1","done":1427694027030}
{"desc":"","position":-7.44415420293808,"last_edited":1427812873054,"task_id":"df533abc-721e-429e-b8e6-6685f3ebf44a","deleted":true}
{"desc":"#today#0\n\n","position":-7.444155544042587,"last_edited":1427638995522,"task_id":"24feae13-3bcc-4c8a-ae41-77b077b4eb06","deleted":true}
{"desc":"#install #5\nbokeh request: d3-based python graphics http://bokeh.pydata.org/en/latest/\n\n - https://mail.google.com/mail/u/2/#inbox/14c6302477f502a4","position":-7.444154649972916,"last_edited":1427812365484,"task_id":"53b103ab-7f69-44ee-8983-fa5292442666"}
{"desc":"(0:31) (0:10?) #1 #today\nterms of service -- cremona says \"I fell asleep before I got very far into this, but did notice some references to \"Sage\" which should presumably be changed to \"SageMath\".   \n\nhttps://mail.google.com/mail/u/2/#inbox/14c4cd26fbc6731e","position":-7.4441556866513565,"last_edited":1427925152112,"task_id":"4f719729-032b-4387-8092-794c6a22efbf","done":1427925151704}
{"desc":"(0:30?) #3\nhome directory permissions in bup_storage.py\n\nSee https://mail.google.com/mail/u/0/?pli=1#inbox/14c685fb7a00d98e\n\n    The problem was that the permissions on the HOME directory of your project were wrong.   I have no idea how this could happen - I've never run into this before with SMC.   This is something that a user could cause by typing \"chmod u-rwx $HOME \" in a terminal, so it might have somehow accidentally been caused by a student.  In any case, nothing is deleted and it's fine now.\n\n    I plan to make a change to the project restart script so that it explicitly fixes the permissions, so this exact problem is much less likely to be a problem in the future. \n","position":-7.444155655801296,"last_edited":1427685145190,"task_id":"fff41e93-ae58-4815-b332-ac315485afa2"}
{"desc":"#bug #ui #9\nin firefox at certain zoom, the top navbar gets too tall\n\nhttps://mail.google.com/mail/u/2/#inbox/14c6884b59edaf9a","position":-7.444155637174845,"last_edited":1427812344404,"task_id":"7523d7aa-4c64-4171-92f4-c11015703481"}
{"desc":"#today\n","position":-7.444155674427748,"last_edited":1427688443395,"task_id":"b523d1bc-b232-4e4a-b7f5-7d9dbd4376dd","deleted":true}
{"desc":"(1:30?) #5 #install \nQPA gap package\n\nSee discussion here `[sage-cloud] QPA in SMC`: https://mail.google.com/mail/u/2/#inbox/14c428b7f4e061e5","position":-6.856781005859375,"last_edited":1427813636496,"task_id":"0a1e1ea9-ccec-44a3-a1d4-debc1618c5f7"}
{"desc":"(0:20?) #1 #today\n- [ ] on March 31 -- actually shut off the compute[1-8]dc2 machines.  turn off monitoring too.\n- [ ] note in the screen on cloud3, I'm making a backup... of everything just in case... but that will likely not even get close in time :-(","position":-7.444186061620712,"last_edited":1427907883699,"task_id":"9960ad17-3d38-4c85-a6a9-b6c066e6157e","due_date":1427866833782,"done":1427907883288}
{"desc":"(1:00?) #3\nservices file needs to be changed to ssd's from hd's for cassandra node (so needs to support that)","position":-7.4441556837409735,"last_edited":1427812313493,"task_id":"7c9af2a2-e540-4e55-9247-31253967b7fb"}
{"desc":"(1:30?) #0 #today #passport\nmake passport stuff live, phase 2.\n\n\n- [ ] (1:00?) register apps with each provider and put stuff in database\n\n        update passport_settings set conf['auth']='https://cloud.sagemath.com/auth' where strategy='site_conf';\n\n        update passport_settings set conf['clientID']='...' where strategy='google';\n        update passport_settings set conf['clientSecret']='...' where strategy='google';\n    \n","position":-7.444155688397586,"last_edited":1427768819874,"task_id":"2b3a9f71-b8d7-4981-9021-5b90fb9ca731","done":1427768819469}
{"desc":"(0:21) (0:45?) #0 #today #bug\nrace -- loading account settings can take longer than loading file (?)\n\nI opened a terminal in a fresh browser tab and it had the wrong settings.  This happens every time.  Ugh.","position":-7.444155690725893,"last_edited":1427755735148,"task_id":"077a489d-0bba-4f10-98f1-99b9a8dca5e4","done":1427755734741}
{"desc":"(0:20?) #0 #today\ntry to copy all of gce's bups at least to cloud3.\n\nin progress in screen there.","position":-7.444155684905127,"last_edited":1427907878661,"task_id":"e8788f6b-3998-477a-86b1-61db6a37c8b2","due_date":1427840674325,"done":1427907878256}
{"desc":"(1:13) (1:00?) #sagews #feature #3\nadd insert for modes, including typeset (!) \n\nsee -- https://mail.google.com/mail/u/2/#inbox/14c5767a0bcd2fe2","position":-6.856813132762909,"last_edited":1428689862407,"task_id":"8ee4b83f-1daa-4652-b520-fc617f495e02","done":1428689862003}
{"desc":"(1:00?) #5 #speed\nmake it so downloading SMC page doesn't have to download a separate css for each codemirror mode.","position":1.9296875,"last_edited":1427738803041,"task_id":"e2968de4-759d-41f1-a5fa-800a282a1e91"}
{"desc":"(1:00?) #1 #bug\ncourses with a space in the filename: https://mail.google.com/mail/u/2/#inbox/14c6afc214eb115e","position":-7.443115234375,"last_edited":1427812456800,"task_id":"678da2e9-f515-4ce3-a12b-490327896feb"}
{"desc":"(1:00?) #1 #install\nrequests to install stuff from Anand Surampudi.\n\n- https://mail.google.com/mail/u/2/#inbox/14c6a728ae94ad07\n- https://mail.google.com/mail/u/2/#inbox/14c6f1d3c540cee6","position":-7.4429931640625,"last_edited":1427812570396,"task_id":"867a80df-72cc-4b01-9ed1-3047d6ee60d7"}
{"desc":"","position":-7.44305419921875,"last_edited":1427812976167,"task_id":"6408aa3c-c5ed-42ca-a58f-82aa2534766f","deleted":true}
{"desc":"#0 #today \ngithub oauth issue\n\nsee https://mail.google.com/mail/u/2/#inbox/14c7014976e3d7c2","position":-7.443023681640625,"last_edited":1427810729528,"task_id":"33345261-fb20-461f-988b-69441b7ca64a","done":1427810729118}
{"desc":"#1 #bug (1:00?)\nsafari graphics issue with SVG -- do something about it...\n\nIf client is using Safari, print a WARNING message on rendering SVG, with example of how to get around it and leak to bug.","position":-7.444186317268759,"last_edited":1428069637257,"task_id":"e206124c-3aff-4957-998c-4d9257c5e961"}
{"desc":"(0:30?) #0 #upgrade #today\ncassandra driver.\n\nhttps://mail.google.com/mail/u/0/?pli=1#inbox/14c7533e968b56bc","position":-7.444155685778242,"last_edited":1427951406219,"task_id":"a4d8dcf5-7c20-40a2-a644-169ad073e7f5","done":1427951405808}
{"desc":"(1:00?) #2 #bug #editor #sync \nfix the insanely slow filesystem reverts edits bug\n\nI can replicate this via sshfs right now, and it's surely a sign of something horrible.  CRITICAL.","position":-0.758575439453125,"last_edited":1428355717202,"task_id":"ac03604d-c813-48e2-bae8-bf65b5210584"}
{"desc":"#now (0:15) (0:30?) #0 #bug #sagews #today\nfix `sage_eval` issue\n\nhttps://mail.google.com/mail/u/2/#inbox/14c7b7d4fd43400f","position":-7.444186279550195,"last_edited":1428071804189,"task_id":"7d98c429-1108-4df0-9ac7-db2cf2c8241d","done":1428071803782}
{"desc":"(1:30?) #1\nstart recording account_id in file history recorder\n\ncrucial to implement and start doing this now, so we have the data for when there is a feature to use it later!","position":-6.856901109218597,"last_edited":1428355706534,"task_id":"4a9ec448-5c05-40d3-aeae-dd81baa66c02"}
{"desc":"(0:32) (0:45?) #bug #sagews #1\ninteract checkboxes broken!\n\nhttps://cloud.sagemath.com/projects/4a5f0542-5873-4eed-a85c-a18c706e8bcd/files/support/2015-04-03-122812-checkboxes.sagews\n\nhttps://mail.google.com/mail/u/2/#inbox/14c80bfc05e66b9c","position":-7.44418616220355,"last_edited":1428544704820,"task_id":"9b72841f-7e75-44ec-a20f-984971bff449","done":1428544704406}
{"desc":"(5:00?) #ipython\ndeal with large images embedded in ipynb files better for sync\n\nSee, e.g., https://mail.google.com/mail/u/2/#inbox/14c834545efef7c5 from Julia Chadwick' via Help on Apr 4, 2015\n","position":-7.444186287932098,"last_edited":1428165005597,"task_id":"d678db7e-6068-4552-a0ae-8ef26bbab014"}
{"desc":"#install #ipython #8\nihaskell\n\nI tried for over an hour and failed before.  Maybe try again:\n\nHaskell kernel support - the ubuntu-install script fails to build some graphical tools (hence semicolon below). This build line takes about an hour!?\n\n\t\tgit clone http://www.github.com/gibiansky/IHaskell && cd IHaskell && ./ubuntu-install.sh; ./build.sh ihaskell; sudo cp /home/salvus/.cabal/bin/ihaskell /usr/local/bin/ && ihaskell install && export PATH=/usr/local/bin/:$PATH && ihaskell install && sudo cp -rv /home/salvus/.ipython/kernels/haskell/ /usr/local/share/jupyter/kernels/\n        \n\nFails at this when trying to build the display stuff.  Doing just `./build.sh ihaskell` above isn't enough, since ihaskell won't actually work as an ipython kernel.  It needs the display stuff. \n\n        CMD: cabal install --constraint arithmoi -llvm -j ./. ./ihaskell-display/ihaskell-aeson ./ihaskell-display/ihaskell-basic ./ihaskell-display/ihaskell-blaze ./ihaskell\n        -display/ihaskell-charts ./ihaskell-display/ihaskell-diagrams ./ihaskell-display/ihaskell-hatex ./ihaskell-display/ihaskell-juicypixels ./ihaskell-display/ihaskell-ma\n        gic ./ihaskell-display/ihaskell-parsec ./ihaskell-display/ihaskell-plot ./ihaskell-display/ihaskell-rlangqq ./ihaskell-display/ihaskell-static-canvas ./ihaskell-displ\n        ay/ihaskell-widgets --force-reinstalls --max-backjumps=-1 --reorder-goals\n        Resolving dependencies...\n        cabal: Could not resolve dependencies:\n        trying: ihaskell-0.6.0.0 (user goal)\n        trying: base-4.6.0.1/installed-8aa... (dependency of ihaskell-0.6.0.0)\n        next goal: ihaskell-static-canvas (user goal)\n        rejecting: ihaskell-static-canvas-0.1.0.0 (conflict:\n        base==4.6.0.1/installed-8aa..., ihaskell-static-canvas => base>=4.7 && <4.9)\n        Dependency tree exhaustively searched.","position":-7.4441862921230495,"last_edited":1428878876334,"task_id":"69ecb5d6-078a-47e8-bdca-790c2276957d"}
{"desc":"#ipython\nideas for improvement\n\n- [ ] handle all the other meta information in sync-friendly way.\n- [ ] currently just replacing cells on change rather than modifying in place -- slower, but maybe way more robust?!  changing in place doesn't work at all right now.  See `if false and cell? and cell_data.cell_type == cell.cell_type`\n- [ ] chat\n- [ ] periodically watch for changes in the actual .ipynb file: this will require rewriting how local_hub works and is nontrivial>\n\n","position":1.9365234375,"last_edited":1428355534229,"task_id":"7eaf410c-5b5d-4540-b5fe-6a665fbffbaa"}
{"desc":"(3:00?) #upgrade #1\nupgrade to sage-6.6\n\nconfirm this fixes graphics in %load_ext sage.","position":-0.7585468292236328,"last_edited":1428355584168,"task_id":"d2c87527-bfa8-47f3-afe0-48ca028a66e6"}
{"desc":"(3:00) (2:30?) #upgrade #0 #today\npush out my new jupyter stuff...","position":-7.444186237640679,"last_edited":1428538533005,"task_id":"5e9b841e-dc8b-4951-86a8-5294f223a9f5","done":1428538532598}
{"desc":"#bug #install\nrstan is evidentally broken again.  Harald wrote about it here:\n\nhttps://mail.google.com/mail/u/1/#inbox/14c8e45717d1136e","position":-0.7585525512695312,"last_edited":1428358583061,"task_id":"cce0c98a-1043-49aa-86bf-89529a0d1813"}
{"desc":"#1 billing analytics\nsee https://mail.google.com/mail/u/1/#inbox/14c88ec7c5994b03","position":-0.7585487365722656,"last_edited":1428362937357,"task_id":"2896e758-8479-4094-b2c4-47df49637efe"}
{"desc":"#course\nauto-collect time\n\n> Set an auto-collect time for an assignment once you make it. That is the one \"workflow\" feature I feel like is \"missing\" for me right now.\n\nSee https://mail.google.com/mail/u/1/#inbox/14c81b3bc6282f28","position":-0.7585506439208984,"last_edited":1428363002339,"task_id":"cc291994-7f52-4637-b225-f3525dd82232"}
{"desc":"#course\ngrading scoring\n\n> Easily assigning a score to an assignment while you have it open, and then getting out a spreadsheet (or CSV, etc) file at the end of the term might be a good first approximation to record-keeping.  You'll have to decide how far down this road you want to go (drop low score, partial credit if late, extra credit assignments,...).\n\nSee https://mail.google.com/mail/u/1/#inbox/14c81b3bc6282f28","position":-0.7585515975952148,"last_edited":1428363031285,"task_id":"c99e23ea-0bb6-4aae-9a77-a95ff25299fc"}
{"desc":"#install #python\ninterpy package (easy pip thing; harder to use for anything useful...) -- https://github.com/syrusakbary/interpy","position":-0.7585563659667969,"last_edited":1428503838160,"task_id":"549ad613-9132-4c89-8a97-470c4c407ee3"}
{"desc":"(0:08) (0:10?) #today\nupdate help.html stats/counts\n","position":-7.4441862313542515,"last_edited":1428539629501,"task_id":"4addeb90-9a84-4f9a-90a6-3c6792a0f5b7","done":1428539629094}
{"desc":"#1 \nemail invitations -- \n\n> Seeing this, it occurs to me that it would be interesting to save what people write in their invite messages (since they are interesting candid sales pitches for SMC!), and also see if I can change the reply-to address to the sender rather than help@sagemath.com, at least if the sender has an email registered with SMC. \n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c96ee188205683","position":-7.44418618734926,"last_edited":1428878894065,"task_id":"6635288a-fe59-4eda-b72c-a6379c739fc8","done":1428878893656}
{"desc":"(0:45?) #3\nadd more oauth options:\n\n- [ ] twitter\n- [ ] bitbucket\n- [ ] dropbox","position":-6.856901399791241,"last_edited":1428539660209,"task_id":"a756b9aa-6091-44b1-bdd0-791d5c08af71"}
{"desc":"(0:20) (1:00?) #today #0 #optimize\nchange sageserver startup to not import lots of stuff and see what a change it makes regarding load time and memory.","position":-7.444155691890046,"last_edited":1428593803772,"task_id":"f422cd6f-cd23-44d8-8819-83cbfb790746","done":1428593803369}
{"desc":"#2 \ndokuwiki questions\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c99e966642256c","position":-7.444155691307969,"last_edited":1428590736524,"task_id":"745d7c39-bb89-4326-a50c-13aa44f34cf1"}
{"desc":"#1 (1:00?)\nmuch better mathjax CSS styling for dark background\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14c9b3947adb9e61","position":-7.444155691599008,"last_edited":1428590802240,"task_id":"18276ec8-02be-4ec4-8cdf-1cad0c551b8f"}
{"desc":"(0:20?) #0 #today #bug #invalid\ntab cycle keyboard keystrokes are reversed...\n\ni suddenly can't replicate this.  Will keep my eyes out.","position":-7.444155691744527,"last_edited":1428609412747,"task_id":"25357d5a-e743-496c-aaa6-091604edd8cf","done":1428609412345}
{"desc":"#now (0:30?) #0 #today\nfix latex filename issue\n\nhttps://mail.google.com/mail/u/0/#inbox/14c9acdda41158f2","position":-7.444155691671767,"last_edited":1428603977295,"task_id":"7b85c4c2-2ab7-46ea-8d8e-9efc96e154ea","done":1428603976892}
{"desc":"(1:00) #today (0:35?)\nadd a new reply-to and bcc email for sagemathcloud invitations.","position":-7.444155691708147,"last_edited":1428609352489,"task_id":"bb4dd102-ad43-461a-9433-09de5438e99a","done":1428609352078}
{"desc":"#editor\nhtml markdown etc editor preview zoom.\n\nThis div needs a zoom button (in SMC): salvus-editor-html-md-preview-content\ntwo buttons next to that preview button, whcih appear when the preview box is checked.  to increase/decrease font-size.\n\nAlso, zoom should get saved in local storage.","position":-7.4441556914534885,"last_edited":1428610051157,"task_id":"cc69259a-06c1-4cc3-938c-9e88d579fe7f"}
{"desc":"(0:44) (1:30?) #today #0\nrewrite ipython daemon script properly to get the pid without calling ps, etc.\n\n- Let's do this right. ","position":-7.444155691380729,"last_edited":1428622156288,"task_id":"544be16a-27b8-49a0-b393-f61cf6df2799","done":1428622155875}
{"desc":"(1:00?) #jupyter #1 #bug\n2 people editing at once -- cursor looses focus","position":1.937255859375,"last_edited":1428941947253,"task_id":"23c2315d-ca7b-418f-a7ed-4f7cda411d10"}
{"desc":"delet","position":-7.444155691417109,"last_edited":1428687667277,"task_id":"90cedcbf-8790-40d6-ae8f-20733cd8bc01","deleted":true}
{"desc":"#1 (0:30?) #bug\narchive program doesn't support straight tar!","position":-6.85690145008266,"last_edited":1428789506245,"task_id":"5a8ccba2-9201-4344-97c0-d884f6c72911"}
{"desc":"(0:30?) #bug #2\nproject is opening message kept appearing repeatedly during my talk.\n\nNever show it more than once if project open!","position":-7.444186304695904,"last_edited":1428878857973,"task_id":"464bfc44-e0ad-47fd-bab3-887b85a81b8b"}
{"desc":"(0:30?) #3 #course #bug\nbug in help text\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14caaabdbcd8d7c6\t\n\n    Sean Raleigh <sean.raleigh@gmail.com>\n    Apr 11 (2 days ago)\n\n    to help \n    Hi William,\n\n    When I go to \"Help\" in the courses area, it brings up a box that is longer than the screen. I can use my mouse wheel to scroll down and read all of it, but I can't drag the scrollbar on the right side of the screen. I can see the scrollbar, but I can't click anywhere to grab it.\n\n    Let me know if I need to send a screencast to see what I'm seeing.\n\n    Thanks,\n    Sean","position":1.93701171875,"last_edited":1428942712017,"task_id":"ef05ae40-fcf1-4cf6-8c6b-eccdd42b283c"}
{"desc":"(0:25+) (2:30?) #5 #support\nhow to add a 3d background?\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14caa0fea9c20e9a\n\nAnswer is here: it's fairly nontrivial, involving changing how rendering works, etc.  About a 2 hour project.","position":1.9373779296875,"last_edited":1428953066167,"task_id":"9bd6dd14-3c84-4d73-b90d-35107b19bb5a"}
{"desc":"(0:30?) #2\ncomment on discussion forum suggestion\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14ca97e40961b6ac","position":-7.44418632959605,"last_edited":1428951630903,"task_id":"752bb018-2f23-49a3-9a20-46efc37da60c"}
{"desc":"(0:30?) #2\nrespond to this email about Learning Management systems\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14ca41f5e6cfd0b9","position":-7.444186329587865,"last_edited":1428951633719,"task_id":"312bbe10-a4a8-43b8-b1e6-c453a944856e"}
{"desc":"(3:00?) #1 #com\ndeploy vm's for commercialization\n\n- [ ] decide on what exactly\n- [ ] put in conf file\n- [ ] udpate template vm\n- [ ] actually turn them on and start them running","position":-0.7585983276367188,"last_edited":1429157091860,"task_id":"159de824-6451-4f3d-8895-9cd05dab854e"}
{"desc":"(0:20) #now (1:00?) #0 #com #today\nupdate stripe api: https://stripe.com/docs/upgrades?since=2015-01-11#api-changelog\n\nin account settings there is a big red button to do the upgrade.","position":-7.444186329690183,"last_edited":1428959328524,"task_id":"fd0e3db7-6de3-4b8d-9ddd-36cfdc1044d6","done":1428959328109}
{"desc":"(0:58) (0:30?) #0 #com #now\nget testing stripe stuff from last month to at least work again in devel1dc5 server.\n\nended up researching tax stuff a lot too.","position":-7.444186329694276,"last_edited":1428957443554,"task_id":"dabe9971-a4b2-4f5c-b21f-614494175065","done":1428957443151}
{"desc":"#now (1:00?) #0 #com #stripe #today\nadd location (country, state) maybe need actual address, so we can charge WA state sales tax properly... if necessary? YES. \n\n- Excellent article: http://www.shopify.com/blog/16480780-should-you-be-charging-sales-tax-on-your-online-store\n- Plus it will be good info to have.\n\nThis site looks nice but doesn't seem to support stripe: http://www.taxjar.com/\n\nStripe sites: https://support.stripe.com/questions/custom-fields-for-tax-tips-shipping-and-more\n\n- [x] (2:37) select country\n- [x] if country == US, show option to select state, and if state = WA, show option to enter zip.","position":-7.44418632968609,"last_edited":1428985761282,"task_id":"aac238cb-fa4d-4f35-8b8a-d535f556f271","done":1428985760874}
{"desc":"(1:30?) #1 #com\nimplement sales tax calculator\n\n(see task in business)","position":-0.7585906982421875,"last_edited":1429157098799,"task_id":"37a232f1-907b-4f4d-ae6e-f1368cf1cc23"}
{"desc":"(in progress) #today #0 #com\nmake plan to move SMC entirely out of UW\n\n- [ ] convert everything that I transferred to google cloud storage as bups to the new btrfs format in parallel\n- [ ] for every project that didn't successfully transfer, run a different conversion that uses rsync directly on last place where project was used.\n- [ ] update all projects from last 7 days\n- [ ] stop on UW, copy, update, start.\n\n\nhttps://console.developers.google.com/project/sage-math-inc/storage/browser/smc-gb-storage/0c47b639-4567-4e8f-a993-ac0fb73132cc/?authuser=1\n\nPLAN?\n\nhttps://cloud.sagemath.com/projects/92848d19-8432-46c8-ba59-2b0d9521c9f2/files/smc/business-model/pricing/2015-04-14-082244.sagews\n\t26gb ram, 4 cores, 375gb local ssd, few hundred gb slow hdd, maybe 100gb swap (?)\n    \n- Investigate/test NVMe support and swap. Maybe requiring building new image or ...\n\n---\n\nHSY: today I did read on google's compute blog, that they offer a 75% reduction for their fast SSD disks. what a coincidence!","position":-0.7585763931274414,"last_edited":1430486849749,"task_id":"4ad1dec2-38bd-4d8e-81b5-36cd5d0dd376","done":1430486849321}
{"desc":"#today #0 (4:00?) #storage\nadd archive functions to `bup_storage.py` that takes a project and produces two files that don't contain extraneous crap (see http://unix.stackexchange.com/questions/28976/how-to-xz-a-directory-with-tar-using-maximum-compression):\n\n\ttar cvf - --exclude=e2542254-f834-4351-9a69-28c50b0ee6e2/.sagemathcloud  e2542254-f834-4351-9a69-28c50b0ee6e2 | lrzip -l -  > foo.tar\n\n> Harald: .xz is slow, it uses single threaded compression. lrz has better ratio and is faster.\n> Harald: The idea behind lrzip is, that there is a large binary (tar-ball) and it is in a first pass searched through for common parts (rolling checksum? no idea what actually). this uses a lot of memory, but it takes care how much is available and so far never crashed the machine. then it lzma (or lzo) compresses the parts in parallel and merges everything together into one .tar.lrz file. `man lrzip` \n\n- [ ] `project-project_id.tar.?`\n- [ ] `bup-project_id.tar.?`\n\n\t\tbup_storage.py archive-project project-id\n\t\tbup_storage.py archive-bup project-id        \n\nMy plan is to \n\n- [ ] upload archives of all projects to a google cloud storage bucket called projects\n- [ ] upload archives of all bups to a google cloud storage bucket called bups\n\nBackup will involve simply copying any new archives to an offsite (in my house) USB disk as well, etc.\n\nRunning SMC will mean `n` machines that have access to the cloud storage buckets.   \n\nOpening a project will:\n1. check if files already available locally in /projects and /bups, with a version that is at least as new as what's in cloud storage\n2. if not locally available, will download and extract files.\n3. then run exactly as usual.\n\nThe above will happen on two machines, so that while project is active, it is regularly mirrored to other machines for redundancy when machines go down, exactly as is the case now.\n\nPeriodically -- say once per week -- every project that has changed gets its archive updated and posted.\n\nWhen projects not used for a certain amount of time $T$ days, it will get deleted from local cache right after successful archive and upload.   For-pay projects will never be archived.\n\nThis separates storage from compute, while providing excellent speed in practice.  Running a projct on a dedicated machine is also easier to implement.\n\nThe one drawback is that a project that hasn't been opened for $T$ days will open possibly much more slowly.  But that's it.    And the cost to store a typical unused 400MB compressed (say) project will be a penny a month.  A 5GB compressed project is maybe $.20/month.   4TB of project data is 2000*0.27 = \\$54/month, which is fine.\n\nHSY: [gsutil subdirectory](https://cloud.google.com/storage/docs/gsutil/addlhelp/HowSubdirectoriesWork):\n`gsutil cp your-file gs://your-bucket/abc/` will create `gs://your-bucket/abc/file`\n\nHSY: [gsutil custom metadata](https://cloud.google.com/storage/docs/gsutil/addlhelp/WorkingWithObjectMetadata#custom-metadata) `gsutil -h x-goog-meta-reviewer:jane cp mycode.java gs://bucket/reviews`","position":-0.7586021423339844,"last_edited":1429283217208,"task_id":"1ba3d6be-3577-42aa-99ec-85b33acaa2a2","done":1429283216792}
{"desc":"#install\nfigure out how to typeset chinese in LaTeX\n\nsee https://mail.google.com/mail/u/1/#inbox from  <xujiahao@pku.edu.cn>","position":-0.7586002349853516,"last_edited":1429156982318,"task_id":"062f763f-185c-4182-812c-4032ab7d0f9e"}
{"desc":"#1 #speed\nAccording to the logs, the function `user_search` in cassandra.coffee is resulting in up to 3 second delays blocking the server.  This is sometimes a serious problem.  Usually though it isn't blocking (I checked more).  Hmm.","position":-0.758601188659668,"last_edited":1429283244086,"task_id":"72dc824d-b14d-4d41-8307-876c68fdbac3"}
{"desc":"#today #0 (5:00?)\nmake bup_storage.py use BTRFS instead of ZFS\n\n- [ ] (1:00?) instructions to create filesystem from device (in comment)\n- [ ] (1:00?) allocating space for project (decide on whether to have a subvolume for each project or not; quota)\n- [ ] (1:00?) should snapshots be per project or across all projects\n- [ ] (1:00?) getting current usage for status\n- [ ] (1:00?) replication -- just stick with rsync or use ","position":-0.7586007118225098,"last_edited":1429473249742,"task_id":"a6451dc4-8f32-4a1c-9597-89e95c6460cb","done":1429473249333}
{"desc":"#now #today #0 (3:00?)\nadd subcommand to bup_storage.py to copy archive to google cloud storage and vice versa\n\n    bup_storage.py gs_sync [project_id]\n    \n- [x] (1:53) (2:00?) Figure out which of gcloud, archive, live is newest.\n  - live: generate archive and copy to gcloud.\n  - gcloud: copy to local archive, then extract to live\n  - archive: copy to gcloud\n\n- [x] (0:23) (1:00?) Also a command\n\n    \tbup_storage.py gs_sync_all \"\"\n\nthat syncs every project that is hosted locally on this machine.\n\n","position":-0.7586009502410889,"last_edited":1429305311278,"task_id":"7cc8b28a-f0fd-4c6a-b1bb-349bda75d781","done":1429305310875}
{"desc":"(5:00?) #0 #today (?)\nimplement gb_storage\n\n- [ ] #critical -- we *must* close before opening a project if it has been opened elsewhere then saved!  Otherwise nothing will receive.\n- re-implement all functionality from bup_storage.py that we need in gb_storage.py\n- also adapt bup_server accordingly.\n- [x] new schema:\n        alter table projects add      gb_location     uuid;\n        alter table projects add      gb_last_save    map<uuid, timestamp>;\n        alter table projects add      gb_size_kb      int;","position":-0.7585773468017578,"last_edited":1429795345457,"task_id":"1bc355ff-07c3-4203-b4e7-fed086a4ade8","done":1429795345047}
{"desc":"#0 (3:00?)\nlive project migration\n\nimplement final migration process for projects modified in the last week\n\n- mark something in database to say project is being moved (and implement some UI to indicate this)\n- find where project last running\n- stop project\n- do a save\n- tar up bup repo\n- copy it to a google node /archive directory\n- gcloud_sync it\n- start project running there and mark database to say so.","position":-0.7585783004760742,"last_edited":1429474557022,"task_id":"3d285a5c-a20e-4fc4-8749-8b236ad45393","deleted":true}
{"desc":"#0 (3:00?)\nfix all the bup repos that are exposed as broken/corrupt by the migration script!","position":-0.7586008310317993,"last_edited":1429473241306,"task_id":"59be9bf7-9b75-4500-a6db-82a35de74eba","deleted":true}
{"desc":"(0:30?) #today #now\nat least ","position":-0.7586008906364441,"last_edited":1429305261359,"task_id":"a775134e-3171-447e-8c6b-0ce77682815f","deleted":true}
{"desc":"#1 #jupyter #install\nextensions\n\n- https://github.com/ipython-contrib/IPython-notebook-extensions\n- http://ask.sagemath.org/question/26586/ipythonjupyter-extensions/","position":-0.7586004734039307,"last_edited":1429365965043,"task_id":"984df704-e147-470f-9e78-82ffd937e4ed"}
{"desc":"(2:20) (2:00?) #0 #today #now\nimplement ASAP fast parallel migration from bup to btrfs format for projects\n\n-  shutdown storage machine and attach and mount those disks with /archive on btrfs-test\n- gsutil: read list of all projects migrated to btrfs so far\n- gsutil: read list of all available bup projects\n- in parallel using async.mapLimit, run archive command on everything not migrated so far.\n\nDo this on `n1-highcpu-32` with local SSD for \\$1.40/hour.  Should take maybe 10 hours (?).\n\n\nhttps://console.developers.google.com/project/sage-math-inc/storage/browser/smc-gb-storage/0c47b639-4567-4e8f-a993-ac0fb73132cc/?authuser=1\n\nNOW RUNNING on 2  8 cpu machines... since was getting tons of gcloud errors with 32 threads from one machine (some sort of anti-dos measures?).","position":-0.758577823638916,"last_edited":1429497637238,"task_id":"d79d16a5-3d18-4982-ba3e-5743a107ebc0","done":1429497636826}
{"desc":"#1 (1:00?)\nsetup my laptop so it can use `gsutil rsync` to start backing up the official smc-gb-storage bucket offline. ","position":-0.7585759162902832,"last_edited":1429617998805,"task_id":"5af4c4c5-14f7-4644-849d-3e2b582cca64"}
{"desc":"#0 #today\nrun parallel migration of everything with valid bup repo that we got.\n\n\nNOW RUNNING on 2  8 cpu machines... since was getting tons of gcloud errors with 32 threads from one machine (some sort of anti-dos measures?).","position":-0.7585768699645996,"last_edited":1429550318617,"task_id":"fdd89e86-7ce2-4f55-aeac-915a3e20fbe9","done":1429550318199}
{"desc":"(2:27) (1:00?) #0 #today #migrate #now\nimplement live update via rsync of projects\n\n\tgb_storage.py migrate_live [--port=port] [hostname:path] [project_id]\n\nThis will do the following:\n\n- open project\n- do rsync (with --excludes) from hostname:path on given port to working directory of project\n- save project\n- close project\n\nBasic stuff done.  Now also need code to dump a file consisting of the following\n\n\tproject_id  last_modification_timestamp  hostname\n    \nNote -- if the server_id is the one for compute16dc0, then get the other hostname.\n\n\tx={};s=require('bup_server').global_client(keyspace:'salvus', cb:(err,c)->console.log(\"err=\",err);x.c=c;x.c.overview_projects(bad_servers:['0985aa3e-c5e9-400e-8faa-32e7d5399dab'], cb:(e,v)->console.log(\"DONE\",e);x.v=v)) \n\nThen I'll run something that goes through every project if either (1) it is not in GS, or (2) it was modified in the last two weeks (say), then calls migrate_live on it, leaving the project open.  Do this on a btrfs with a lot of disk space, so can update it periodically easily.","position":-0.7585756778717041,"last_edited":1429550363942,"task_id":"448faa66-54f8-4215-9339-1474a66c0d00","done":1429550363537}
{"desc":"(1:20) (2:00?) #0\nfix cassandra blob issue\n\nsee gitter:\n\n> @mkarakoc -- I added \"plt.savefig('a.svg')\" before the plt.show() to save it to this file: https://cloud.sagemath.com/projects/5e66cbfa-b823-4679-811f-36d7b511e995/files/sage_codes/stream_plot_electricCharges_2014_finalSinavi.sagews\nhttps://cloud.sagemath.com/projects/5e66cbfa-b823-4679-811f-36d7b511e995/files/sage_codes/a.svg\nAt least then you can view the image.\nThe BLOB size issue is because there is something broken about the database driver that makes BLOBs lock the server for about 2s/MB while saving, which is VERY bad for usability. I'll fix that locking issue sometime today (I hope!), and then the blob limit can be raised back to 10MB.","position":-0.7585755586624146,"last_edited":1429745523192,"task_id":"2e23948b-7166-4595-9d9c-37174e9fb006","done":1429745522783}
{"desc":"(1:00?) #today #0\n\n- [x] command that gets all projects modified in the last week and updates them is cloud storage\n\n\t\tx={};s=require('bup_server').global_client(keyspace:'salvus', cb:(err,c)->console.log(\"err=\",err);x.c=c;x.c.overview_projects(bad_servers:['0985aa3e-c5e9-400e-8faa-32e7d5399dab'], cb:(e,v)->console.log(\"DONE\",e);x.v=v))","position":-0.7585771083831787,"last_edited":1429575819818,"task_id":"a4330a3a-2073-480e-b1dd-9c18f72b9c79","done":1429575819390}
{"desc":"(1:30?) #today #0\nparallel command that goes through all projects that aren't in gs and copies them from live then DELETES them from local machine, unless modified in last week.","position":-0.7585775852203369,"last_edited":1429575830258,"task_id":"06b3e049-7a92-42f3-a2ff-1cc2e9fab30b","done":1429575829836}
{"desc":"#today\nbtrfs issues to watch out for (?)\n\nHit this: https://btrfs.wiki.kernel.org/index.php/Problem_FAQ#I_get_.22No_space_left_on_device.22_errors.2C_but_df_says_I.27ve_got_lots_of_space\n\nI fixed it by doing\n\n    btrfs fi show\n    btrfs fi df /projects\n    btrfs fi balance start -dusage=5 /projects\n\nand deleting things until the above worked.  Then doing this freed way more space\n\n    btrfs fi balance start -dusage=50 /projects\n\nas explained here: http://marc.merlins.org/perso/btrfs/post_2014-05-04_Fixing-Btrfs-Filesystem-Full-Problems.html.   This issue is space wasted by btrfs but not actually used/reported in df. \n\nThis is relatively fast -- a few seconds to 45 seconds.","position":-0.7585769891738892,"last_edited":1429575805342,"task_id":"b97bf062-e561-4801-94bb-0b16e38607ba"}
{"desc":"(0:45?) #today\nimplement gb storage auto-snapshot, which continuously keeps new storage up to date with projects in cluster.","position":-0.7585774660110474,"last_edited":1429583560648,"task_id":"c1487f23-6f6d-41b8-8435-d6f0bbede02e","done":1429583560213}
{"desc":"(1:00?) #1 #upgrade\nupgrade mathjax -- https://mail.google.com/mail/u/1/#inbox/14cd85cfb97129fd\t","position":-7.444155691526248,"last_edited":1429617371557,"task_id":"f7981c5f-f457-44bb-9b0e-0a9877476e71"}
{"desc":"(1:00?) #1 \nraise the public file size download limit from 20MB to 50MB (?). \n\nSee https://mail.google.com/mail/u/1/#inbox/14cd6342188c15f4\n\nand in particular, this file: https://cloud.sagemath.com/projects/19d1c156-e6da-41a4-899e-11346c774f9f/files/uber_4weeks.csv","position":-7.444155691489868,"last_edited":1429617646992,"task_id":"ff37bd8e-d768-4607-bc6e-3264c66d5dba"}
{"desc":"(1:00?) #today #0 #gb_storage #now\nquota","position":-0.7585772275924683,"last_edited":1429727924804,"task_id":"500c4f4c-bfe3-4b0e-8729-9c242416259d","done":1429727924394}
{"desc":"(0:45?) #1\nsingle ip address quota issue\n\nhttps://mail.google.com/mail/u/1/?pli=1&zx=v6crnjw8ww15#inbox/14cdc5a71fcb2a5e","position":-0.7585754692554474,"last_edited":1429745774724,"task_id":"87d9bc52-5b4f-45fc-897f-b9314523ff11"}
{"desc":"(0:45?) #0 #today #storage\ncontrol groups quota\n","position":-0.758577287197113,"last_edited":1429799178152,"task_id":"86ee034f-ac23-47c2-86e0-f7b39968f04a","done":1429799177735}
{"desc":"(0:40) (0:45?) #0 #today #storage #now\nnetwork quota","position":-0.7585772573947906,"last_edited":1429804498372,"task_id":"4cd9f0d7-9173-4eeb-b1b5-a536fa714751","done":1429804497965}
{"desc":"(2:00) (3:00?) #0 #today #storage\n(re-)write smc_compute-- make a plan\n\nWhat does this massive smc_compute thing need to do?  Right now it is a mix of project replication, location, state change, copying files around, etc.  Functionality:\n\n- status of project: current state and info about\n- start project running on optimal host\n- stop project, restart project\n- snapshot and save project\n- delete snapshot\n- delete project\n- read file/directory\n- copy file directory between projects\n\nCurrent used api -- get a single project and call these functions\n\nProjectClient:  (runs on hub)\n- open: open project on given node\n- start: start project running\n- close: kill everything and remove project from this compute node\n- move: move project from this compute node to another one\n- restart: kill all processes, then start key daemons\n- stop: kill all processes\n- save: create snapshot, save incrementals to cloud storage\n- local_hub_address: project location and listening port\n- status: information about project (ports, state, etc.)\n- state: just the state of the project, which is one of: \n   closed, opened, running,\n   opening, starting, restarting, stopping\n   error\n- copy_path: copy a path using rsync from one project to another\n- read_file: read a file or directory from disk\n- set_settings: set various quotas\n\nComputeServerClient:   (runs on hub)\n- add_server\n- servers: compute server id's and health/load info\n- call: send message to a server and get back result\n\nComputeServer:  (runs on compute server)\n- project_command: run a command for a project (error if not allowed now due to state)\n- project_state: returns state of a project on this node\n","position":-0.7585772424936295,"last_edited":1429976661183,"task_id":"521a7bfb-4474-4e30-89b6-67590bdef750","done":1429976660769}
{"desc":"(1:00?) (0:55) #0 #today #storage #now\nreimplement copy_path","position":-0.75857724994421,"last_edited":1429816615853,"task_id":"83c8abbc-1766-4abe-a1ad-323010a657db","done":1429816615449}
{"desc":"(0:25) (0:45?) #0 #today #storage #now\nreimplement read_file\n","position":-0.7585772518068552,"last_edited":1429812261432,"task_id":"462c1426-2eeb-4971-8761-14c623fe0059","done":1429812261017}
{"desc":"(0:17) (0:45?) #0 #today #storage #now\nreimplement directory_listing","position":-0.7585772536695004,"last_edited":1429808711825,"task_id":"30740e8f-018f-4311-b984-7b591d25af6d","done":1429808711412}
{"desc":"(2:30) (0:45?) #0 #today #storage #now\nway to make tarball archive of project as a just-in-case backup that is safer than btrfs stream, and can be used for export to user; properly excludes.\n\n> ended up doing this very nicely with nearline long-term incremntal backups not using btrfs, in case of corruption or other issues.  Will cost very little to store for a long time, is safety against snapshot deletion, etc.","position":-0.758577243424952,"last_edited":1429884058544,"task_id":"4c53f1fe-a5e1-4c9a-b180-8a2d5f062c64","done":1429884058126}
{"desc":"#3\nsome build help requests\n\nhttps://mail.google.com/mail/u/2/#inbox/14ce52ed2fbb1ee5","position":-0.758577246684581,"last_edited":1429813283007,"task_id":"71fd00a8-836f-471d-b0b6-f31765ae707f"}
{"desc":"(2:30?) #2 \nasync-ish JSON\n\nSee \"Asynchronous JSON functionality\" in `misc_node.coffee`","position":-0.7585772429592907,"last_edited":1429845379284,"task_id":"d93c9385-6590-42cc-92e0-fe937642e10b"}
{"desc":"(0:23) (0:45?) #0 #today #compute #now\ncompute -- client/server tcp connection","position":-0.7585766315460205,"last_edited":1429982058970,"task_id":"a832f927-f84b-4bb8-91ae-a1c0234a34c1","done":1429982058551}
{"desc":"(0:27) (0:30?) #0 #today #compute #now\ncompute -- add_server","position":-0.7585767507553101,"last_edited":1429979231946,"task_id":"3e7572b9-b6e8-401f-b0f7-f66dd899b5de","done":1429979231541}
{"desc":"(0:16) (0:30?) #0 #today  #compute #now\ncompute -- send message to server/get back result","position":-0.7585765719413757,"last_edited":1429984662258,"task_id":"58b419ad-2eef-49a7-ac7a-8a6f4be95dbf","done":1429984661851}
{"desc":"#now (0:48) (0:30?) #0 #today #compute\ncompute -- run a project command","position":-0.7585764229297638,"last_edited":1429987588808,"task_id":"377bb0fd-c996-4090-9abf-c829b086119b","done":1429987588403}
{"desc":"(0:13) (0:30?) #0 #today #compute #now\ncompute -- get project state","position":-0.7585764080286026,"last_edited":1429988649468,"task_id":"ae567f6f-5428-4002-992b-2fba3f353e75","done":1429988649062}
{"desc":"(1:27) (0:45?) #0 #today #compute #now\ncompute -- impose proper locking on project commands","position":-0.7585763959214091,"last_edited":1430011292699,"task_id":"a0d2541d-50d3-45af-a9cb-2cd15cb0c248","done":1430011292293}
{"desc":"(0:24) (0:30?) #0 #today #compute #now\ncompute -- open project\n\n(and a bunch more)","position":-0.7585763968527317,"last_edited":1429990104101,"task_id":"adcb8270-3002-41b5-99bd-3417cc7edfe5","done":1429990103689}
{"desc":"(0:30?) #0 #today #compute\ncompute -- start project deamons running","position":-0.7585763949900866,"last_edited":1430018879068,"task_id":"491cc452-8fdd-434a-b626-5b071fdea4a4","done":1430018878657}
{"desc":"(0:30?) #0 #today #compute\ncompute -- close project ","position":-0.758576394058764,"last_edited":1430018881968,"task_id":"7cd2f133-6499-4fca-b363-3e4bdf36c916","done":1430018881560}
{"desc":"(0:30?) #0 #today #compute\ncompute -- move project","position":-0.7585763935931027,"last_edited":1430018882500,"task_id":"c552f7f5-860a-420f-ae99-80aa48251eb0","done":1430018882096}
{"desc":"(0:30?) #0 #today #compute\ncompute -- restart project","position":-0.758576393360272,"last_edited":1430018884122,"task_id":"2944c6a1-502a-46e5-8fb4-c39debfacca1","done":1430018883348}
{"desc":"(0:30?) #0 #today #compute\ncompute -- stop project","position":-0.7585763932438567,"last_edited":1430018885907,"task_id":"476edbd9-9176-48d1-b549-f9bb212e5915","done":1430018885493}
{"desc":"(0:30?) #0 #today #compute\ncompute -- save project","position":-0.7585763931856491,"last_edited":1430018886789,"task_id":"d05c5a07-4852-4557-baa9-03343bb68c7c","done":1430018886385}
{"desc":"(0:10) (0:30?) #0 #today #compute #now\ncompute -- address","position":-0.7585763931565452,"last_edited":1430019501303,"task_id":"716433ce-7d1c-4dde-ab9c-6e62ff119836","done":1430019500899}
{"desc":"(0:30?) #0 #today #compute\ncompute -- project status","position":-0.7585763931419933,"last_edited":1430019527325,"task_id":"75c24f35-7581-49d8-b17b-775d7d519c0a","done":1430019526913}
{"desc":"(0:30?) #0 #today #compute\ncompute -- project state","position":-0.7585763931347174,"last_edited":1430019529244,"task_id":"7830c0ce-46c1-4889-9cdb-b6e65b5c4096","done":1430019528830}
{"desc":"(1:29) (0:40?) #0 #today #compute\ncompute -- copy path","position":-0.7585763931310794,"last_edited":1430025536402,"task_id":"70cc45c2-2edf-4df9-9dd2-b2c857347fe4","done":1430025535996}
{"desc":"(0:40?) #0 #today #compute\ncompute -- read file","position":-0.7585763931292604,"last_edited":1430025544121,"task_id":"71d0e0e6-07f3-438b-be7e-8c5a0bdb1515","done":1430025543701}
{"desc":"(0:29) (1:00?) #0 #today #compute #now\ncompute -- set quotas","position":-0.7585763931306246,"last_edited":1430027302605,"task_id":"5291d643-7cc1-4bdf-809b-438d4e04b1d1","done":1430027302191}
{"desc":"(4:04) (2:00?) #0 #today #compute\ncompute/hub -- switch hub to use new compute server\n\n- [x] rewrite local hub to async make project on need.\n- [x] implement compute project restart.\n- [x] rewrite all instances of local_hub.project or local_hub?.project to be calls on local_hub\n- [x] implement delete_project in compute.coffee\n- [ ] test that hub's delete_project round line 5178 actually works\n- [x] #now min save interval: make it so save doesn't happen more than it should","position":-0.7585763931289193,"last_edited":1430079468974,"task_id":"7d9940e9-147d-491d-98ac-fb90461b1fa4","done":1430079468552}
{"desc":"(0:08) (0:20?) #0 #today #compute #deploy\n\nupdate db schemas:\n\n        alter table projects add compute_server   varchar;\n        alter table projects drop gb_location;\n        alter table projects drop gb_last_save;\n        alter table projects drop gb_size_kb;\n    \n    \n    CREATE TABLE compute_servers (\n    host          varchar PRIMARY KEY,\n    port          int,\n    dc            varchar,\n    health        float,\n    secret        varchar,\n    experimental  boolean             \n    );\n\n","position":-0.7585763931276688,"last_edited":1430324238883,"task_id":"8403d196-66a6-490f-b2c2-67dec50f0656","done":1430324238469}
{"desc":"(0:18) (0:20?) #0 #today #compute #now\nmake tcp connection robust -- if socket dies, remove from cache.","position":-0.7585766017436981,"last_edited":1429983384043,"task_id":"1a322cce-8159-4e76-b53f-d3961225c9bc","done":1429983383634}
{"desc":"(0:45) (0:45?) #0 #today #compute #now\nmake it so global hubs are *notified* on state change by a project. Then they can make proper decisions.","position":-0.7585763954557478,"last_edited":1430015735334,"task_id":"9198447a-ac6c-4fec-9d79-424b0f0cfcf8","done":1430015734931}
{"desc":"(0:43) (0:45?) #0 #today #compute #now\ncompute -- choose a host on which to open project / use the one mentioned in the database.","position":-0.7585763931301699,"last_edited":1430029912544,"task_id":"37605079-a4fd-41bc-afb6-e23658fe9bfe","done":1430029912135}
{"desc":"(0:37) (0:30?) #0 #today #compute #now\nmove project","position":-0.7585763931297151,"last_edited":1430032121808,"task_id":"d45c4e88-178f-44a2-b8e9-9245991941f8","done":1430032121383}
{"desc":"(2:00?) #0 #today #compute\ntest new code on a multinode devel setup (should create a new virtual network)","position":-0.7585763931285783,"last_edited":1430087773479,"task_id":"d5a7a3d4-d757-4b64-b2b8-f3e87de94857","deleted":true}
{"desc":"(1:00?) #0 #today #compute\ntransition the live cluster over... \n\n- [x] 4 machines: n1-highmem-4 in the haswell DC; called compute1234-us-central1-c\n- [x] put machines in Route 53 DNS called compute1, compute2, compute3, compute4\n\n- [x] comment this in hub before going live, since this can make things slow: `DEBUG = true`","position":-0.758576393127445,"last_edited":1430516770298,"task_id":"5c333aed-f707-4b9f-9306-05ecfad1bb00","done":1430516769884}
{"desc":"(3:15) (1:00?) #0 #today #compute #now\nimplement project idle timeout\n\n(took a while because I also added sqlite state to db)\n\noptions:\n\n- define idle to mean that the touch time in the database hasn't changed\n- or could define idle to mean last snapshot, which is impacted by touch time.\n- then compute server could just check for each running project what last snapshot time is.  If it is too long ago, stop project.  in that case, we will need a way to persist the idle timeout settings locally since compute server can be restarted and we have no local file anymore.\n- doing things globally would have to be poll-based rather than event driven... and it's impossible to coordinate which hub would do the work. \n- really should persist compute server stuff anyways:\n    - state of each project -- so we know what is running without having to use ps\n    - idle timeouts\n    - could later use it as a local definitive log of project activity... on that ephemeral-ish node.\n    \n# SCHEMA\n\n   \tproject_id     state      mintime\n   \n    CREATE TABLE projects (project_id TEXT PRIMARY KEY, state TEXT, mintime INTEGER)\n   \n\n# try it\n\n    npm install sqlite3 \n\n- on startup:\n    - read each state\n    - for any non-stable state, run smc_compute to find state (or maybe just declare corrupt depending on what state it was in?)","position":-0.7585763931288881,"last_edited":1430181878309,"task_id":"1b15ac7d-5dc9-4f3f-b044-1b282d7ce7fc","done":1430181877895}
{"desc":"(0:18) (0:30?) #now #0 #today #compute\nchange compute.coffee get_project cache to not be tricked by two clients getting project at same time (so lock and call all callbacks).","position":-0.7585763931288879,"last_edited":1430110198150,"task_id":"1119302f-bd53-4277-87fb-f2dbf6dba41f","done":1430110197734}
{"desc":"(0:20?) #0 #today #compute\nclient ui -- change client to use new state (state, time) and not use get_local_state\n","position":-0.7585763931288341,"last_edited":1430087681360,"task_id":"08ba5ef3-d056-4c06-bac6-c45c29ec9fcf","done":1430087680951}
{"desc":"(0:20?) #0 #today #compute\nclient ui -- change client to use .snapshots directory instead of .snapshots/master","position":-0.7585763931287914,"last_edited":1430087748701,"task_id":"04a03b47-c63a-415d-9d2d-fbe948c86a95","done":1430087748291}
{"desc":"(0:13) (0:30?) #0 #today #compute #now\nsetup path and bashrc and so on properly...","position":-0.7585763931288885,"last_edited":1430097878763,"task_id":"662cadaa-5bc6-4caa-80ef-bfa5c914c6c3","done":1430097878355}
{"desc":"(0:45) (0:15?) #0 #today #compute #now\nclient ui -- change location in project settings to hostname\nchange ssh instructions.\n\nended up fixing location and directories","position":-0.758576393128898,"last_edited":1430090521357,"task_id":"b3f02d10-f851-4f75-b01a-34a884f79433","done":1430090520947}
{"desc":"(3:07) (1:30?) #0 #today #compute #now\nsetup a second compute node and test:\n- [x] making project on it\n- [x] moving project to it\n- [x] what is needed to configure it\n- [x] what happens with project when it vanishes/dies -- need auto failover","position":-0.7585763931288835,"last_edited":1430157863316,"task_id":"fcb5cae5-3e81-411d-b4ba-c864a077a6bd","done":1430157862910}
{"desc":"(0:47) (0:30?) #0 #today #compute #now\nrewrite display of all quotas in project settings \"status.settings\", and of course include this info with status message.\n\n- [x] in project.coffee and html, change explicit zfs stuff\n- [x] setting of quotas","position":-0.7585763931288863,"last_edited":1430107874555,"task_id":"6c126ce2-7253-48b3-86e8-4d1ee1809274","done":1430107874143}
{"desc":"(0:33) (0:30?) #0 #today #compute #test #now\nsystematically test all public stuff.\n\n- [x] auto-zip of directories\n- [x] ipython\n- [x] sage worksheets\n- [x] making things public\n- [x] auto-opening a project to show...  (works sort of but is crappy)","position":-0.7585763931288927,"last_edited":1430097088068,"task_id":"90a5d399-483e-4c58-9997-167e762c7ddc","done":1430097087662}
{"desc":"#now (0:04) (0:30?) #0 #today #compute #test\ntest copying paths\n\n- [x] file between projects and within\n- [ ] course assignments\n- [x] copying to a project that is closed\n","position":-0.7585763931288934,"last_edited":1430094933402,"task_id":"dd9140c4-a066-41a5-afbd-a97f1c0a5493","done":1430094932982}
{"desc":"(0:32) (0:30?) #0 #today #compute #now\nfix that raw url is now broken:\n\nhttps://devel1dc5.sagemath.com/c47701bf-26da-43da-a51d-14b83c3f8e88/raw/","position":-0.7585763931288954,"last_edited":1430093417129,"task_id":"3b8f062c-75ad-4cc1-b084-b3069b1edbd7","done":1430093416723}
{"desc":"(0:13) #now (0:20?) #0 #today #compute\nadd more warnings about sagemathcloud path being ephemeral","position":-0.758576393128894,"last_edited":1430094076673,"task_id":"93511e4b-5bf6-468f-b059-b535f5c23d33","done":1430094076270}
{"desc":"(4:23) (1:30?) #0 #today #compute #security\ncreate firewall rules script.\n\nfigure out how to make it so that nothing except port 22 listening on eth0 is visible to *any* compute nodes.    In particular test this for the raw server, jupyter, etc.\n\n- when creating compute machines in GCE, need to properly tag them so that raw server isn't exposed to other compute machines\n\n- basically only traffic allowed between compute machines should be port 22 -- absolutely nothing else.\n","position":-0.7585763931288863,"last_edited":1430420890030,"task_id":"8f0316d6-f26f-4678-be91-e541dc3cfcf9","done":1430420889605}
{"desc":"(0:10?) #0 #today #compute\ncomment this in hub before going live, since this can make things slow:\n\n\tDEBUG = true\n","position":-0.7585763931274982,"last_edited":1430108005881,"task_id":"0578a002-9c71-4f95-9a9d-66b5b452b7b9","deleted":true}
{"desc":"(0:15?) #0 #today #compute\ntest that old snapshots get deleted automatically","position":-0.7585763931288879,"last_edited":1430107902426,"task_id":"ef5c60e1-85c5-4262-a37a-129d58fa8506","deleted":true}
{"desc":"(0:30?) #2 #compute\nauto open/start project when raw server requested (?)\nmaybe not...?","position":-0.758576393127484,"last_edited":1430181946892,"task_id":"6810ebf4-f061-4ff5-b4fe-8cd3173abeaf"}
{"desc":"(1:00?) #2 #ui #bug\nopening a pdf shows up tiny -- refresh and is big\n\nprobably a when displayed versus timing thing...","position":-0.75857639312889,"last_edited":1430097067131,"task_id":"b7034770-b3aa-440a-85a0-2ff98bba6bc8"}
{"desc":"#1 #compute\nautomate this somehow (from smc_compute.py) whenever local_hub code is updated and redeployed:\n\n\tsudo rsync -LrxvH --delete /home/salvus/salvus/salvus/local_hub_template/ /projects/sagemathcloud/\n","position":-0.7585763931288887,"last_edited":1430097770588,"task_id":"1c028f25-ba4b-45c8-9a5d-1a6525008e1c"}
{"desc":"(0:46) (0:45?) #0 #today #compute #now\ndisabling network access break connecting to the local hub","position":-0.7585763931288849,"last_edited":1430327767405,"task_id":"aa20f2fb-9192-43d7-a62f-267eca6ae8a9","done":1430327767003}
{"desc":"(1:18) (0:30?) #0 #today #compute\nactually set all the settings when project starts; also improve ui and add memory info.","position":-0.7585763931288879,"last_edited":1430143959113,"task_id":"52127e9d-8855-437e-825a-955ce0e6fb38","done":1430143958701}
{"desc":"(2:00?) #1\nwhen disk is full, make the gui delete tool still work by not using trash, etc.\n\nwhen disk is full make border in red around project or something with clear indicator... and link to upgrade disk space or delete stuff.","position":-0.7585763931288894,"last_edited":1430107604799,"task_id":"8c6a8beb-5e77-4418-b9bd-693fd58ab7c4"}
{"desc":"#2 #compute\nmake number of snapshots a setting/quota","position":-0.7585763931288814,"last_edited":1430107930252,"task_id":"21e8524d-16fc-4e1e-8a10-92a7f4475f88"}
{"desc":"(1:00?) #2 #compute\nreturn total used memory according to `smem -ntk` as part of status?","position":-0.7585763931288825,"last_edited":1430109863941,"task_id":"d1b420eb-65ba-412a-9cb2-76d39b871491"}
{"desc":"(0:20?) #0 #today #compute #now\ntest -- make quota setting work even if project isn't opened/running","position":-0.7585763931288845,"last_edited":1430145360166,"task_id":"3a17853c-7c93-47be-87ff-ca8eccb483a4","done":1430145359757}
{"desc":"(0:27) (1:00?) #0 #today #compute #now\ncompute client: state sync needs to be rock solid.  it isn't right now.\n\nBasically the state must stay sync'd.\n\nSurprisingly wasn't only due to a bug in caching...","position":-0.7585763931288867,"last_edited":1430336290301,"task_id":"50173598-aeba-40b2-92ac-32bf24f82231","done":1430336289889}
{"desc":"#now (1:30?) #0 #today #compute\nautomate adding/remove computing nodes\n\nAlso configuration steps right now:\n\n\t# first some fixes:\n\n        salvus@admin1dc5:~/salvus/salvus/local_hub_template$ scp devel1dc5:salvus/salvus/local_hub_template/data.tar.bz2 .\n        salvus@admin1dc5:~/salvus/salvus/local_hub_template$ scp data.tar.bz2 compute1-us-central1-c:salvus/salvus/local_hub_template/\n        and extract it!\n        salvus@admin1dc5:~/salvus/salvus/local_hub_template$ ./make_coffee\n        chmod a+rx /usr/local/sage\n        \n\n\t# now real stuff\n\n    cd salvus/salvus\n    . salvus-env\n    git fetch\n    git checkout wstein/gb-storage\n    ./make_coffee\n\tsudo cp scripts/smc_compute.py /usr/local/bin/\n\tsudo rm /usr/local/bin/bup_storage.py\n\t\n    # as sudo\n    export DEV=/dev/sdb; mkfs.btrfs $DEV && mkdir -p projects && mount -o compress-force=lzo,noatime $DEV /projects && btrfs quota enable /projects/ && chmod og-rw /projects && chmod og+x /projects\n    \n    \n\tbtrfs subvolume create /projects/sagemathcloud && sudo rsync -LrxH --delete /home/salvus/salvus/salvus/local_hub_template/ /projects/sagemathcloud/\n\tbtrfs subvolume create /projects/tmp && chmod 1777 /projects/tmp && mount -o bind /projects/tmp /tmp/\n    \n    # as salvus user\n    echo 'export SMC_BTRFS=/projects-btrfs; export SMC_BUCKET=gs://smc-gb-storage-devel; export SMC_ARCHIVE=gs://smc-gb-archive-devel' >> $HOME/.bashrc\n    \n\n","position":-0.7585763931288879,"last_edited":1430478674068,"task_id":"88c4ee89-1647-4c63-8540-65abd03c788d","done":1430478673655}
{"desc":"(0:20?) #0 #today #compute\nhub program.port -- register server is failing since program.port is strangenly null.","position":-0.758576393128885,"last_edited":1430155226758,"task_id":"ef7d9271-c0d6-4bbc-b792-0ecfc4a821d9","deleted":true}
{"desc":"(0:26) (0:20?) #0 #today #compute #now\nset a quota on the .sagemathcloud directory -- otherwise it is a loophole.","position":-0.7585763931288898,"last_edited":1430162302838,"task_id":"1b19a7fd-e41c-491c-a6c6-e7e58487615a","done":1430162302418}
{"desc":"(1:30?) #0 #today #compute\nautomatic failover\n\n- [x] (1:15) (0:30?) when opening a project on a node where not already opened, we need to clear space first... in case something was left laying around.   Way to do this: have when project was opened be stored in the database.\n\n- [x] (1:30) (0:45?) #now status info directly from compute servers, and summary version got in parallel by client.\n\n- [x] (1:16)  (0:45?) automatically move off a node that isn't responding to a new node that is working\n\n","position":-0.7585763931288897,"last_edited":1430356064840,"task_id":"273d292c-2630-455d-bd20-f0603ddda6ee","done":1430356064425}
{"desc":"(1:00?) #2 #dos #compute \nlook into installing some sort of temp watcher or quota on tmp (?)","position":-0.7585761547088623,"last_edited":1430166400404,"task_id":"5e1a382e-1029-48b3-a665-71ed0c2637ef"}
{"desc":"(0:42) (0:20?) #0 #today #now\nmake ~/.snapshots read only\n\nalso fixed some project settings displays","position":-0.7585763931288886,"last_edited":1430166354621,"task_id":"c2397cfb-dec0-47fc-8e02-68a11a8492ad","done":1430166354208}
{"desc":"(0:22) (0:10?) #0 #today #compute #now\nrestart project server spinner","position":-0.7585763931288891,"last_edited":1430163639829,"task_id":"b3692077-8819-4b34-a4a6-e8228616cf85","done":1430163639418}
{"desc":"#2 #btrfs\nsomehow change everything related to project file copying to use `--reflink=always`, which possible.","position":-0.7585763931288882,"last_edited":1430161271066,"task_id":"a18e9244-a755-4285-bc55-8251be18f967"}
{"desc":"(2:00?) #2 #btrfs\nuser option to try to run dedup to free up space.","position":-0.7585763931288884,"last_edited":1430161444936,"task_id":"957f62a5-3f43-45c8-856b-43b8dc6405a0"}
{"desc":"(2:00?) #compute #1\nhave a quota/setting that ensures a project is always running \n\nThis would work just like mintime with a similar interval in compute.","position":-0.7585763931288896,"last_edited":1430182305721,"task_id":"f380d9a0-f1cc-413c-b499-11d995444d77"}
{"desc":"(0:34) (0:30?) #0 #today #compute #now\nidle timeout may as well throw in a save before timing out project.","position":-0.7585763931288888,"last_edited":1430323799029,"task_id":"68f2534f-1543-4be8-a533-217c7efa5f6d","done":1430323798624}
{"desc":"(1:00?) #3 #compute\nwhat about  1-month idle timeout that closes project?","position":-0.7585763931288854,"last_edited":1430185969950,"task_id":"79603561-02bf-4903-a8dc-f0a2ceeb7b8e"}
{"desc":"(1:00?) #3 #bug #sagews\naxiom interface fix from bill page: https://mail.google.com/mail/u/1/#inbox/14cf859ed06547a3","position":-0.7585763931288846,"last_edited":1430186122517,"task_id":"eb2903a3-3f06-475e-be05-93b180b41228"}
{"desc":"#2 #bug #serious\nruning ipython if you've never run sage in a project breaks startup!\n\nSee email from Ryan Nelson: https://mail.google.com/mail/u/1/#inbox/14d05bd5d29cb36d","position":-0.758576393128888,"last_edited":1430320827701,"task_id":"62dcb2ba-6957-411a-9ee6-243f6bfdf688"}
{"desc":"(0:15?) #0 #today #compute\nchange code in client to make ssh name be everything before dash","position":-0.7585763931288853,"last_edited":1430334439273,"task_id":"ba0d93ef-a695-4abf-a8f1-fb1ebf69a0ff","deleted":true}
{"desc":"#0 #today\nbetter way of opening firewall to gce web server nodes that is automated.  Right now I just add this to the ip_whitelist file:\n\n\techo 'import os; print(\"\\n\".join([os.popen(\"nslookup smc%sdc%s |grep \\\"Address: 10\\\"\"%(i,j)).read().split()[-1] for i in range(1,7) for j in [5,6]]))' | python\n","position":-0.7585763931288899,"last_edited":1430327761421,"task_id":"54a6dacd-122a-426f-a032-a7c7581e67c8","deleted":true}
{"desc":"(0:08) (0:30?) #0 #today\nproject deletion traceback issue","position":-0.7585763931288895,"last_edited":1430328230990,"task_id":"601030dd-fc31-4c70-8f4f-ad4312835ddc","done":1430328230571}
{"desc":"(0:14) (0:45?) #0 #today #compute\nget move to actually work reliably.  It might already (?).","position":-0.7585763931288879,"last_edited":1430337185231,"task_id":"3b5e8b2f-3e81-450c-90aa-58f688eebf29","done":1430337184816}
{"desc":"#3\nmake it so logs don't get copied (by default) when pushing out assignments\n\nhttps://mail.google.com/mail/u/1/#inbox/14d07107ef5cb155\t\n\nCasey Greene","position":-0.7585763931288867,"last_edited":1430342831127,"task_id":"76c7f0d2-60b8-40b9-bbe5-e941d8a758a8"}
{"desc":"(1:00?) #1 #compute\nway for compute server to remove all projects that shouldn't be there \n\n(according to global hub database).  this will only happen when a compute server goes down, projects move, but can't be removed, so compute server thinks they are still there. ","position":-0.7585763931288848,"last_edited":1430349985496,"task_id":"c41dcee3-db8b-4d93-b0e2-caee04d885c1"}
{"desc":"#1 #compute\noptimization idea -- maybe store the btrfs subvolume creation time in the database (?) if possible, since it can take a long time to compute; it could just be a cache.\n\n     time btrfs subvolume show /projects/.snapshots/....\n     \nThis is dominating the migration stuff right now...     ","position":-0.7585763931288889,"last_edited":1430356079063,"task_id":"ab58a0e6-c2e8-4995-a270-e1f0f329ef88"}
{"desc":"(0:14) (0:30?) #0 #today\nincrease core quota for these students temporarily: \n\n   https://cloud.sagemath.com/projects/ddbe2a04-de0d-4d69-b9fb-46ccb356dded/files/2015_Spring.course\n   \n   https://mail.google.com/mail/u/1/?ui=2&view=btop&ver=15iwqxmrupss2&search=inbox&th=14d081ead9414b29&cvid=5","position":-0.7585763931288879,"last_edited":1430441065104,"task_id":"b971c2c7-25e7-47d0-ad79-49b51fb558ef","done":1430441064693}
{"desc":"(0:20?) #0 #bug \nget rid of stupid \"(this takes about 30 seconds)\" opening project message -- useless and far too frequent","position":-0.7585754841566086,"last_edited":1430486854133,"task_id":"011a24f1-5891-4276-9bc4-0674cf45435f","done":1430486853717}
{"desc":"(3:47) (1:30?) #0 #today #security #compute #now\nUse the new firewall script\n\n- [ ] compute machines: outgoing on start of compute server based on google metadata server (?)\n\n- [ ] compute machines: incoming to secure non-secure services like ipython\n- [ ] compute machines: change smc_compute script to call smc_firewall for adding/removing users from firewall... or  change compute.coffee to directly call smc_firewall for firewall action (since it will already be calling it for the above 2, right?).\n- [ ] frontend web machines: run smc_firewall to allow incoming connections on ports 443,80,22 and from all other web machines -- maybe use google metadata server for this.\n\n\nMetadata server:\n\nSetting it takes a while (13seconds!):\n\n    time gcloud compute project-info add-metadata --metadata incoming_whitelist_hosts=smc1dc5,smc2dc5,smc3dc5,smc4dc5,smc5dc5,smc6dc5,smc1dc6,smc2dc6,smc3dc6,smc4dc6,smc5dc6,smc6dc6,devel1dc5\n    \nAny server at all in this gce project can then query the metadata instantly (1/10th of a second):\n\n    time curl \"http://metadata.google.internal/computeMetadata/v1/project/attributes/incoming_whitelist_hosts\" -H \"Metadata-Flavor: Google\"\n    \n    \nso hard...    ","position":-0.7585763931288879,"last_edited":1430439861813,"task_id":"06cb798c-8223-4517-8949-dbe51052fb79","done":1430439861402}
{"desc":"(1:00?) #2 #gce\noptimization -- when sagews2pdf runs it downloads from the blob store via sagemath.com.  \nIt should use an address that is internal to GCE somehow... (no idea how).","position":-0.7585763931288867,"last_edited":1430417045801,"task_id":"72b94898-18ae-45bf-93c1-366445699c8c"}
{"desc":"(0:18) (0:30?) #0 #today #now\n- [x] reset sendgrid password -- https://mail.google.com/mail/u/2/#inbox/14d0ba81d6419fd4\n- [x] change it on my hosts and in the template","position":-0.7585763931288892,"last_edited":1430422299957,"task_id":"dde652f9-327b-4c9b-954d-736c9d5a08aa","done":1430422299551}
{"desc":"#0 #now\nDeploy!\n\n- [x] create machine(s) compute0-us-central1-c\n\n- [x] port migrate script over\n\n- [x] rewrite \"created by\" part of script to be much more efficient\n\n- [x] #now run migrate to get all projects changed in last 24 (?) hours to be on this machine and updated from live\n\n- [x] create script that automates preparing the local scratch ssd (see below)\n\n- [x] create compute1-us-central1-c, compute2-us-central1-c, compute3-us-central1-c\n\n- [ ] update dns\n\n- [x] NO -- too hard/dangerous/scary -- make it so right after the switch, when we *first* open a project it does one last rsync, then switches a field in the database (unset bup_location), so it won't do that anymore.  This way we just have to open randomly all projects from the last week, and then we can safely shut down the cluster.\n\n- [ ] client ui -- make display of disk space be in MB instead of GB.\n\n- [x] restart webservers to use new code\n\n","position":-0.7585763931288864,"last_edited":1430573835833,"task_id":"d87fe015-67a4-4769-b05a-6f8e0db3dd17","done":1430573835414}
{"desc":"#0 #now\nautomate conf of local-ssd\n\n-- now in `scripts/gce/init-compute-instance` -- still have to manually run it, but at least it automates the confusing error-prone part.\n\n    root@compute0-us-central1-c:/home/salvus/salvus/salvus/scripts# sfdisk -d /dev/sdb\n    # partition table of /dev/sdb\n    unit: sectors\n\n    /dev/sdb1 : start=      256, size= 20971520, Id=82\n    /dev/sdb2 : start= 20971776, size= 77332224, Id=83\n    /dev/sdb3 : start=        0, size=        0, Id= 0\n    /dev/sdb4 : start=        0, size=        0, Id= 0\n    root@compute0-us-central1-c:/home/salvus/salvus/salvus/scripts# sfdisk -d /dev/sdb  > sfdisk-sdb-ssd.layout\n    root@compute0-us-central1-c:/home/salvus/salvus/salvus/scripts# chown salvus. sfdisk-sdb-ssd.layout\n    root@compute0-us-central1-c:/home/salvus/salvus/salvus/scripts# exit\n    salvus@compute0-us-central1-c:~/salvus/salvus/scripts$ git add sfdisk-sdb-ssd.layout","position":-0.7585763931288879,"last_edited":1430487601912,"task_id":"20ed2a5c-006c-4786-bb4f-d6b5af272f5b","done":1430487601503}
{"desc":"(1:30?) #1 #compute\nwrite code to remove older projects from compute machine\n\n- [ ] function in smc_compute.py to list all projects that haven't had a snapshot since a given timestamp.\n\n- [ ] new function in compute.coffee to \"close all projects\" that haven't been snapshotted since a timestamp.   Critical thing is that we must not do a save first if the project has been opened elsewhere due to failover.  We can detect this by checking in the .opened file and seeing whether there is soemthing newer in GCS: if yes, dust delete, but if no save first, then delete.","position":-0.7585763931274432,"last_edited":1430512331946,"task_id":"9a2e7448-6905-4db7-833a-395125da7600"}
{"desc":"#0 #today\n\n- [ ] add new  web/frontend/cassandra nodes in haswell data center as a new data center to the cassandra cluster... (?)\n\n    smc0-us-centra1-c\n    smc1-us-centra1-c\n    smc2-us-centra1-c\n\neach should be n1-highmem-2 with ~ 100GB persistent SSD (?). Format disk using btrfs, so I can snapshot if I want.\n\n","position":-0.7585763931288856,"last_edited":1430528929840,"task_id":"8c85d3c9-ce36-4369-8938-9e5f60b12e8b","done":1430528929430}
{"desc":"#0 #now\nscript for construction of new vm's\n\n\texport VM=1; export ZONE=us-central1-c; export TYPE=n1-highmem-4\n\n    gcloud compute --project \"sage-math-inc\" disks create \"compute$VM-$ZONE\" --zone \"$ZONE\" --source-snapshot \"compute-2015-05-01-1231\" --type \"pd-ssd\" && gcloud compute --project \"sage-math-inc\" instances create \"compute$VM-$ZONE\" --zone \"$ZONE\" --machine-type \"$TYPE\" --network \"default\" --maintenance-policy \"MIGRATE\" --scopes \"https://www.googleapis.com/auth/devstorage.full_control\" \"https://www.googleapis.com/auth/logging.write\" --disk \"name=compute$VM-$ZONE\" \"device-name=compute$VM-$ZONE\" \"mode=rw\" \"boot=yes\" --local-ssd-count \"1\"\n","position":-0.758576393128889,"last_edited":1430493636779,"task_id":"7cfb4adf-934c-45b5-960e-7322ed3e545b","done":1430493636359}
{"desc":"#0 #now\n","position":-0.7585763931288847,"last_edited":1430492303595,"task_id":"b2e20063-5bd4-4376-935f-093b9e98f0c5","deleted":true}
{"desc":"(2:00?) #1 #compute\n- [ ] compute server: add function to estimate how long it may take to transition from current state to next state, based on some heuristics and how long in current state.\n- [ ] hub: return time estimate along with status info for a project\n- [ ] client: show time estimate in project UI in various ways.","position":-0.7585763931274423,"last_edited":1430573888174,"task_id":"92a1094c-f687-4222-9196-8a5b9a502db5"}
{"desc":"#0\ncassandra\n\n- [x] add three new nodes in haswell europe making a new DC after everything is up and running\n\n- [ ] shutdown all existing smc nodes","position":-0.7585763931288851,"last_edited":1430573936828,"task_id":"351c49c0-e3ff-4ca6-9067-db6e9338f8b5","done":1430573936419}
{"desc":"(0:23) (0:30?) #0 #compute #today #now\n\n- [x] create new compute vm.\n- [x] add https and http traffic to all existing smc vm's\n- [x] change gce.py so when creating new smc vm's allow https and http traffic","position":-0.7585763931288857,"last_edited":1430572736891,"task_id":"9f686045-43c6-4188-bfeb-6527d280669d","done":1430572736487}
{"desc":"#0 #compute #critical\n- [x]  (0:21) the RAM quota units changed from GB in the old version to MB in new, which breaks things completely.\n- [ ] #now increase disk space quotas from old to new due to lack of dedup and compression maybe not counting so much (?).   Or maybe just whenever setting disk quota always go a little over actual usage the first time, no matter what?\n","position":-0.7585763931288867,"last_edited":1430570401506,"task_id":"17889354-6b7e-447c-9bd0-2ea067850c1b","done":1430570401092}
{"desc":"#2 #bug #compute\ncopying files between project to directory that doesn't exist doesn't properly create that directory...","position":-0.7585763931288858,"last_edited":1430573853777,"task_id":"d92d7491-858d-4e6e-bc27-1253104e5c7b"}
{"desc":"#1 #compute\nafter switching over, address this in compute.coffee\n\n    # TODO: this is a temporary workaround until I go through and convert everything in\n    # the database, after the switch.\n    if quotas.memory < 70\n        quotas.memory *= 1000\n\nJust do a linear scan through the db multiplying all memory settings that are less than 70 (say) by 1000...","position":-0.7585763931288863,"last_edited":1430567891685,"task_id":"6809330d-765a-4021-8f74-4acdea3854c0"}
{"desc":"(1:00?) #0 #critical\nshut down two old cassandra data centers.","position":-0.7585754916071892,"last_edited":1430598507875,"task_id":"9ece22c5-3bcc-4280-9934-6b9a0414aac3"}
{"desc":"(1:30?) #0 #monitor #critical\nsetup monitoring of all new machines and shutdown monitoring of old\n- [ ] uptimerobot (very tedious)\n- [ ] google cloud monitoring\n- [ ] aws health checks and dns","position":-0.7585763931288869,"last_edited":1430598499634,"task_id":"5b93c257-e70c-457b-8180-72f087dd840e"}
{"desc":"(0:13) (0:30?) #0 #test #today\n- [x] test that cassandra database works fine with one node down.\n- [x] test automatic failover for projects","position":-0.7585763931288859,"last_edited":1430573454571,"task_id":"cb5aff6b-0b0f-4ffb-894a-ec14887f41d0","done":1430573454161}
{"desc":"#now (1:00?) #0 #critical #bug\nfailover issue -- when failover is initiated all hubs need to kill their connections.\n\nbasically failover happens when compute server communication fails.  But all the tcp connections to  the local project might continue to work just fine.  So the hub needs to periodically check (?) to see if the project is still on the same host.  If not, it must restart.\n\nNeed to test with same project opened by clients using two different hubs.","position":-0.7585763931288867,"last_edited":1430584171842,"task_id":"c37f91fd-5581-4e45-b403-ea90cc986046","done":1430584171422}
{"desc":"(0:30?) #0 #compute #now\n- [x] autostart compute server when compute node boots up (and test)\n- [x] crontab to start it periodically anyways in case killed due to OOM","position":-0.758576393128887,"last_edited":1430575598463,"task_id":"7e3856c0-31af-48d1-8e1f-a80e7190aa4f","done":1430575598057}
{"desc":"(0:30?) #0\nright before switch:\n- [ ] disable compute0\n- [ ] efficiently close all projects there without save\n- [ ] re-enable compute0","position":-0.7585763931288867,"last_edited":1430584188515,"task_id":"c85a7d2e-6185-4af1-9bfb-8151090bf253","done":1430584188078}
{"desc":"(0:15?) #0\nupdate dns to point at new smc servers","position":-0.7585763931288861,"last_edited":1430584190901,"task_id":"fcc4e599-f63d-486f-8db0-42d36f34f7f3","done":1430584190492}
{"desc":"(2:00?) #1\ntry to auto-start smc daemons on bootup of those nodes, based on last configuration used to start them.\n\n- [ ] cassandra\n- [ ] nginx\n- [ ] hub\n- [ ] haproxy","position":-0.7585763931288868,"last_edited":1430575738543,"task_id":"5a378cf2-5ed3-4977-8922-e193b6a3632d"}
{"desc":"(1:00?) #0 #compute\nimplement a clean up tasks that\n\n- [ ] gets list of all projects opened on a given compute host\n- [ ] if any shouldn't be there due to having moved, it closes them (save=false, force=true)\n","position":-0.7585763931288879,"last_edited":1430587711849,"task_id":"fb126d5d-8a56-480b-b089-390ccd0e47b4","done":1430587711428}
{"desc":"(1:00?) #0 #critical\ncheck that all firewalls are properly running and up\n\n> smc one right now breaks proxy server and everything.  \n\n> ok the raw server/stats/latex problems were all caused by the firewall on the webserver nodes blocking connections from localhost, which is something that is needed on the compute vm's ONLY.  So I turned off the firewall on web machines for now (it isn't necessary since everything has strong passwords anyways).","position":-0.758575489744544,"last_edited":1430598510872,"task_id":"c99d0bd0-43bc-4348-abec-f2c6f2db7954"}
{"desc":"#0 #today\nset DEBUG=false for hub at some point","position":-0.7585763931288867,"last_edited":1430584185050,"task_id":"63336f12-a6fd-48a9-8a87-616c365c35db","done":1430584184634}
{"desc":"#0 #compute\ncompute -- allow start while saving (?)","position":-0.7585754878818989,"last_edited":1430590998831,"task_id":"b8a18e1c-c4a3-43ea-a14a-a91614680f9d"}
{"desc":"(1:00?) #0\nraw server broken/flakie/messed up\n","position":-0.7585763931288871,"last_edited":1430590985385,"task_id":"fa2ed8d4-69ce-41ab-b94c-c4cfdee0d273","done":1430590984972}
{"desc":"#0 #critical\nrepair fix db's in europe then restarting hubs there to use them.","position":-0.7585763931288879,"last_edited":1430598489744,"task_id":"fb9ec8f4-1732-4381-a2b8-f5cf595f1e40"}
{"desc":"#0 #critical #bug\ncopying to projects that are closed maybe doesn't work.","position":-0.7585763931288879,"last_edited":1430598495214,"task_id":"430cd26c-801b-4210-bcd9-784c22b9a493"}
{"desc":"#0 #critical #compute\nneed to do the btrfs extraction/creation etc on a different filesystem.  any user could fill /tmp and break saving for everybody!\n\njust make a 100GB SSD PD and use that...? ","position":-0.7585763931288879,"last_edited":1430598481728,"task_id":"44db6204-ae98-4ede-a5e3-b4c2e3241b82"}
{"desc":"#0 #today\nmake it so compute.coffee server waits for status to finish before calling it again.","position":-0.7585763931288879,"last_edited":1430668706689,"task_id":"c738d35c-6913-4352-967e-b9a5b664a750"}
{"desc":"#0 #today\nmake is to client doesn't call status too frequently","position":-0.7585763931288872,"last_edited":1430668724457,"task_id":"67c2abb8-953a-4f1f-a59e-5341d11687f5"}
{"desc":"#0 #today #now\nmake it so that transition states, when read from db, if old are discarded.","position":-0.7585763931288879,"last_edited":1430670081984,"task_id":"110aed8f-aa84-428c-a92a-117d85746cc0"}